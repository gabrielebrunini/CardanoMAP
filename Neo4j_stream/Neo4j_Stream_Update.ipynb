{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84f93e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:39:36.003772Z",
     "start_time": "2023-01-13T16:39:35.675526Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pyspark\n",
    "from pyspark.sql.types import  *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f61d2d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:39:36.019047Z",
     "start_time": "2023-01-13T16:39:36.007209Z"
    }
   },
   "outputs": [],
   "source": [
    "client = MongoClient(\"172.23.149.210\", 27017)\n",
    "db = client['cardano_silver']\n",
    "db2 = client['cardano_bronze']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63851f1f",
   "metadata": {},
   "source": [
    "### Create MongoDB Collections for Address Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required collections with the last checkpoint\n",
    "tx_out = db[\"node.public.tx_out\"]\n",
    "addr_last_ind = db[\"last_index_address\"]\n",
    "\n",
    "# import the required temporary collection to overwrite it with new data\n",
    "addr_tmp = db[\"address_temporary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the last indices that were processed before\n",
    "addr_last_processed = addr_last_ind.find_one({'collection': 'address'})['last_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39412ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many documents are in each new input mongodb collection\n",
    "count_addr = tx_out.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the records which haven't been processed yet (range between addr_last_processed and total records count)\n",
    "addr_df = tx_out.find()[addr_last_processed:count_addr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ccc98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the previous records in the temporary collections\n",
    "addr_tmp.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4325be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the temporary records in the temporary collections\n",
    "addr_tmp.insert_many(addr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e50464c",
   "metadata": {},
   "source": [
    "###  Create MongoDB Collections for Transaction Network Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3b39586",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T17:29:35.899359Z",
     "start_time": "2023-01-12T17:29:35.895249Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required collections\n",
    "tx = db[\"transaction_network\"]\n",
    "tx_last_ind = db2[\"last_index_neo4j_stream\"]\n",
    "\n",
    "# import required temporary collections to overwrite with new data\n",
    "tx_tmp = db2[\"neo4j_stream_temporary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8485c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the last indices that were processed before\n",
    "tx_last_processed = tx_last_ind.find_one({'collection': 'neo4j_stream'})['last_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27456a7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T17:29:58.379035Z",
     "start_time": "2023-01-12T17:29:58.372629Z"
    }
   },
   "outputs": [],
   "source": [
    "# count how many documents are in each new input mongodb collection\n",
    "count_tx = tx.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "857c60f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-17T14:23:29.151084Z",
     "start_time": "2022-12-17T14:23:29.146931Z"
    }
   },
   "outputs": [],
   "source": [
    "# for each Cardano table, select the records which haven't been processed yet (range between last_processed and total records count)\n",
    "tx_df = tx.find()[tx_last_processed:count_tx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d43d2ab8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-17T14:23:31.309186Z",
     "start_time": "2022-12-17T14:23:30.989573Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop the previous records in the temporary collections\n",
    "tx_tmp.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1d50ebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-17T14:23:33.460990Z",
     "start_time": "2022-12-17T14:23:32.188439Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f771d839fd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the temporary records in the temporary collections\n",
    "tx_tmp.insert_many(tx_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dd4e46",
   "metadata": {},
   "source": [
    "### Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28635a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:39:54.502598Z",
     "start_time": "2023-01-13T16:39:54.496982Z"
    }
   },
   "outputs": [],
   "source": [
    "config = pyspark.SparkConf().setAll([\n",
    "    ('spark.executor.memory', '30g'), \n",
    "    ('spark.executor.cores', '5'), # number of cores to use on each executor\n",
    "    ('spark.cores.max', '15'), # the maximum amount of CPU cores to request for the application from across the cluster\n",
    "    ('spark.driver.memory','20g'),\n",
    "    ('spark.driver.maxResultSize', '4g'),\n",
    "    ('spark.executor.instances', '3'),\n",
    "    ('spark.worker.cleanup.enabled', 'true'),\n",
    "    ('spark.worker.cleanup.interval', '60'),\n",
    "    ('spark.worker.cleanup.appDataTtl', '60'),\n",
    "    ('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector:10.0.2'),\n",
    "    ('spark.mongodb.output.writeConcern.wTimeoutMS','1000000'),\n",
    "    ('spark.mongodb.output.writeConcern.socketTimeoutMS','1000000'),\n",
    "    ('spark.mongodb.output.writeConcern.connectTimeoutMS','1000000'),\n",
    "    (\"neo4j.url\", \"bolt://172.23.149.210:7687\"),\n",
    "    (\"neo4j.authentication.type\", \"basic\"),\n",
    "    (\"neo4j.authentication.basic.username\", \"neo4j\"),\n",
    "    (\"neo4j.authentication.basic.password\", \"cardano\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d92ce16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:06.325299Z",
     "start_time": "2023-01-13T16:40:00.995307Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: neo4j.url\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.password\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.type\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.username\n",
      "23/01/13 17:40:01 WARN Utils: Your hostname, cardano-druid resolves to a loopback address: 127.0.0.1; using 172.23.149.210 instead (on interface ens3)\n",
      "23/01/13 17:40:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bd80fdc2-dac5-432c-9714-39c8c427bcd0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector;10.0.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.5.1 in central\n",
      "\t[4.5.1] org.mongodb#mongodb-driver-sync;[4.5.0,4.5.99)\n",
      "\tfound org.mongodb#bson;4.5.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.5.1 in central\n",
      ":: resolution report :: resolve 1540ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.5.1 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector;10.0.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   1   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bd80fdc2-dac5-432c-9714-39c8c427bcd0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/5ms)\n",
      "23/01/13 17:40:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/13 17:40:04 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf=config) \\\n",
    "    .appName(\"Neo4j-Stream-Update\") \\\n",
    "    .master(\"spark://172.23.149.210:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d83c4",
   "metadata": {},
   "source": [
    "#### Define schema for the different collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da62c525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:06.331413Z",
     "start_time": "2023-01-13T16:40:06.328470Z"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"address\", StringType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73629f56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:06.428436Z",
     "start_time": "2023-01-13T16:40:06.332964Z"
    }
   },
   "outputs": [],
   "source": [
    "schema2 = StructType([ \\\n",
    "    StructField(\"address\", StringType(), True), \\\n",
    "    StructField(\"id\", IntegerType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62953510",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:07.890388Z",
     "start_time": "2023-01-13T16:40:07.886606Z"
    }
   },
   "outputs": [],
   "source": [
    "schema3 = StructType([ \\\n",
    "    StructField(\"input_addr\", StringType(), True), \\\n",
    "    StructField(\"output_addr\", StringType(), True), \\\n",
    "    StructField(\"tx_hash\", StringType(), True), \\\n",
    "    StructField(\"input_ADA_value\", IntegerType(), True), \\\n",
    "    StructField(\"output_ADA_value\", IntegerType(), True), \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a45d65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:09.194232Z",
     "start_time": "2023-01-13T16:40:09.191041Z"
    }
   },
   "outputs": [],
   "source": [
    "schema4 = StructType([ \\\n",
    "    StructField(\"hash\", StringType(), True), \\\n",
    "    StructField(\"id\", IntegerType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f3518",
   "metadata": {},
   "source": [
    "### Address Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d984a999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T15:56:12.531421Z",
     "start_time": "2023-01-13T15:56:11.194416Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'address_temporary') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85da919d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T15:56:12.869117Z",
     "start_time": "2023-01-13T15:56:12.534033Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_addresses.createOrReplaceTempView(\"temp_addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aea741d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T15:56:18.181515Z",
     "start_time": "2023-01-13T15:56:18.114525Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_addresses2 = spark.sql(\"SELECT DISTINCT address FROM temp_addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd889087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T15:56:31.100013Z",
     "start_time": "2023-01-13T15:56:21.194717Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/13 16:56:21 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_addresses2.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'address_temporary_2') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f9cc1f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:16:50.790182Z",
     "start_time": "2023-01-13T16:16:49.396189Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'address_temporary_2') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59c4c70c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:16:50.816785Z",
     "start_time": "2023-01-13T16:16:50.793970Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_silver') \\\n",
    "  \t.option('spark.mongodb.collection', 'addresses') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34b9dfab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:16:51.205598Z",
     "start_time": "2023-01-13T16:16:50.862125Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses.createOrReplaceTempView(\"addresses\")\n",
    "temp_addresses.createOrReplaceTempView(\"temp_addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce9b625e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:16:52.148660Z",
     "start_time": "2023-01-13T16:16:52.042402Z"
    }
   },
   "outputs": [],
   "source": [
    "new_addresses = spark.sql(\"SELECT address FROM temp_addresses WHERE NOT EXISTS (SELECT address FROM addresses)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12a89f66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:16:57.606748Z",
     "start_time": "2023-01-13T16:16:52.648108Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/13 17:16:52 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_addresses.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'new_addresses_temporary') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47df216b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:35:57.980643Z",
     "start_time": "2023-01-13T16:35:57.975406Z"
    }
   },
   "outputs": [],
   "source": [
    "# find first address id for new addresses collection\n",
    "addr = db[\"addresses\"]\n",
    "count = addr.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76ba7542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:36:15.807090Z",
     "start_time": "2023-01-13T16:36:15.803142Z"
    }
   },
   "outputs": [],
   "source": [
    "# create ids for the new addresses\n",
    "collection = db2.new_addresses_temporary.find()\n",
    "for doc in collection:\n",
    "    update = {'$set': {\"id\": count}}\n",
    "    db2.new_addresses_temporary.update_one(doc, update)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa65bc90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:36:35.144533Z",
     "start_time": "2023-01-13T16:36:35.124585Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the new addresses with their ids\n",
    "final_new_addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'new_addresses_temporary') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema2) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76931619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append new unique addresses to the existing collection of addresses\n",
    "final_new_addresses.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_silver') \\\n",
    "   .option('spark.mongodb.collection', 'addresses') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fd2c69b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:37:50.773979Z",
     "start_time": "2023-01-13T16:37:46.248347Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/13 17:37:46 WARN Partitioner: Unable to get collection stats (collstats) returning a single partition.\n",
      "23/01/13 17:37:46 WARN Partitioner: Unable to get collection stats (collstats) returning a single partition.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# stream new addresses to neo4j\n",
    "final_new_addresses.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"labels\", \"Address\") \\\n",
    "  .option(\"node.keys\", \"id\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7693a57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:39:10.645132Z",
     "start_time": "2023-01-13T16:39:10.640357Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop temporary collection\n",
    "db2.address_temporary_2.drop()\n",
    "db2.new_addresses_temporary.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update checkpoints\n",
    "addr_query = {\"collection\": \"address\"}\n",
    "new_count = {\"$set\":{\"last_index\": count_addr}}\n",
    "addr_last_ind.update_one(addr_query, new_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc37ae",
   "metadata": {},
   "source": [
    "### Transaction Network & Clusters Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1289b3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:23.492269Z",
     "start_time": "2023-01-13T16:40:22.150041Z"
    }
   },
   "outputs": [],
   "source": [
    "TxNetwork = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_temporary') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema3) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54c9dc17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:26.163784Z",
     "start_time": "2023-01-13T16:40:25.852413Z"
    }
   },
   "outputs": [],
   "source": [
    "TxNetwork.createOrReplaceTempView(\"TxNetwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2e49fb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:26.497469Z",
     "start_time": "2023-01-13T16:40:26.478145Z"
    }
   },
   "outputs": [],
   "source": [
    "tx = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'node.public.tx') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema4) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9e5954e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:30.207817Z",
     "start_time": "2023-01-13T16:40:30.192453Z"
    }
   },
   "outputs": [],
   "source": [
    "tx.createOrReplaceTempView(\"tx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2b415fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:35.561968Z",
     "start_time": "2023-01-13T16:40:35.474068Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id = spark.sql(\"SELECT tx_hash, id as tx_id, input_addr, output_addr, input_ADA_value, output_ADA_value FROM TxNetwork LEFT JOIN tx on TxNetwork.tx_hash = tx.hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "193549ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:41.053909Z",
     "start_time": "2023-01-13T16:40:41.031388Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id.createOrReplaceTempView(\"transactions_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a3eea9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:42.689908Z",
     "start_time": "2023-01-13T16:40:42.671507Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_silver') \\\n",
    "  \t.option('spark.mongodb.collection', 'addresses') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema2) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c803aee8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:45.112126Z",
     "start_time": "2023-01-13T16:40:45.098894Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses.createOrReplaceTempView(\"addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37b62d66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:46.504599Z",
     "start_time": "2023-01-13T16:40:46.478308Z"
    }
   },
   "outputs": [],
   "source": [
    "input_addresses = spark.sql(\"SELECT transactions_id.tx_hash, transactions_id.tx_id, transactions_id.input_addr, transactions_id.output_addr, transactions_id.input_ADA_value, transactions_id.output_ADA_value, addresses.id as input_id FROM transactions_id LEFT JOIN addresses on transactions_id.input_addr = addresses.address\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "104ecf02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:48.241561Z",
     "start_time": "2023-01-13T16:40:48.225454Z"
    }
   },
   "outputs": [],
   "source": [
    "input_addresses.createOrReplaceTempView(\"input_addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4759649d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T16:40:49.107885Z",
     "start_time": "2023-01-13T16:40:49.082026Z"
    }
   },
   "outputs": [],
   "source": [
    "TxNet = spark.sql(\"SELECT input_addresses.tx_hash, input_addresses.tx_id, input_addresses.input_addr, input_addresses.output_addr, input_addresses.input_ADA_value, input_addresses.output_ADA_value, input_addresses.input_id, addresses.id as output_id FROM input_addresses LEFT JOIN addresses on input_addresses.output_addr = addresses.address\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e50718",
   "metadata": {},
   "source": [
    "### Neo4j Tx Network & Clusters Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a742f057",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T17:08:26.071358Z",
     "start_time": "2023-01-13T16:41:16.041639Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/13 17:48:14 WARN TaskSetManager: Lost task 414.0 in stage 1.0 (TID 428) (172.23.149.210 executor 1): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/01/13 17:48:14 WARN TaskSetManager: Lost task 420.0 in stage 1.0 (TID 434) (172.23.149.210 executor 1): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 37 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# stream transaction network\n",
    "TxNet.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.210:7687\") \\\n",
    "  .option(\"query\",\"MATCH (a1:Address {id: event.input_id}) MATCH (a2:Address {id: event.output_id}) CREATE (a1)-[:TRANSACTED_WITH {tx_hash: event.tx_hash, tx_id: event.tx_id, input_ADA_value: event.input_ADA_value, output_ADA_value: event.output_ADA_value}]->(a2)\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dc04f0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T17:08:26.110880Z",
     "start_time": "2023-01-13T17:08:26.089402Z"
    }
   },
   "outputs": [],
   "source": [
    "TxNet.createOrReplaceTempView(\"TxNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1166ebd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T17:08:26.367100Z",
     "start_time": "2023-01-13T17:08:26.113664Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters = spark.sql(\"SELECT t1.tx_id, t1.input_id as input1, t2.input_id as input2 FROM TxNet t1 LEFT JOIN TxNet t2 on t1.tx_id = t2.tx_id WHERE t1.input_id != t2.input_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc9d23c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T17:08:26.385778Z",
     "start_time": "2023-01-13T17:08:26.369844Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters.createOrReplaceTempView(\"clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "949d5524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T17:08:26.433507Z",
     "start_time": "2023-01-13T17:08:26.387201Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters_final = spark.sql(\"SELECT distinct tx_id, input1, input2 FROM clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66407d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream clusering network\n",
    "clusters_final.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.210:7687\") \\\n",
    "  .option(\"query\",\"MATCH (a1:Address {id: event.input1}) MATCH (a2:Address {id: event.input2}) CREATE (a1)-[:ASSOCIATED_WITH]->(a2)\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ae9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update checkpoints\n",
    "tx_query = {\"collection\": \"neo4j_stream\"}\n",
    "new_count = {\"$set\":{\"last_index\": count_tx}}\n",
    "last_ind.update_one(tx_query, new_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c54444e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-17T14:24:26.931563Z",
     "start_time": "2022-12-17T14:24:26.830404Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,auto:percent",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
