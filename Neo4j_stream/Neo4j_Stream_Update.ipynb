{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c84f93e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:11:30.412436Z",
     "start_time": "2023-01-28T13:11:30.408964Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pyspark\n",
    "from pyspark.sql.types import  *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f61d2d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:11:30.905416Z",
     "start_time": "2023-01-28T13:11:30.899743Z"
    }
   },
   "outputs": [],
   "source": [
    "client = MongoClient(\"172.23.149.210\", 27017)\n",
    "db = client['cardano_silver']\n",
    "db2 = client['cardano_bronze']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63851f1f",
   "metadata": {},
   "source": [
    "### Create MongoDB Collections for Address Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "443e2dab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:11:33.226333Z",
     "start_time": "2023-01-28T13:11:33.222391Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the required collections with the last checkpoint\n",
    "tx_out = db2[\"node.public.tx_out\"]\n",
    "addr_last_ind = db2[\"last_index_address\"]\n",
    "\n",
    "# import the required temporary collection to overwrite it with new data\n",
    "addr_tmp = db2[\"address_temporary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5d7ec70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:11:33.955138Z",
     "start_time": "2023-01-28T13:11:33.950192Z"
    }
   },
   "outputs": [],
   "source": [
    "# retrieve the last indices that were processed before\n",
    "addr_last_processed = addr_last_ind.find_one({'collection': 'address'})['last_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39412ceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:11:40.645688Z",
     "start_time": "2023-01-28T13:11:40.641832Z"
    }
   },
   "outputs": [],
   "source": [
    "# count how many documents are in each new input mongodb collection\n",
    "count_addr = tx_out.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f61aa5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:11:43.241367Z",
     "start_time": "2023-01-28T13:11:43.238072Z"
    }
   },
   "outputs": [],
   "source": [
    "# select the records which haven't been processed yet (range between addr_last_processed and total records count)\n",
    "addr_df = tx_out.find()[addr_last_processed:count_addr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40ccc98b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:11:44.206334Z",
     "start_time": "2023-01-28T13:11:44.202735Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop the previous records in the temporary collections\n",
    "addr_tmp.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4325be9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:29:42.113222Z",
     "start_time": "2023-01-28T13:11:44.589274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f210f161f10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the temporary records in the temporary collections\n",
    "addr_tmp.insert_many(addr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e50464c",
   "metadata": {},
   "source": [
    "###  Create MongoDB Collections for Transaction Network Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3b39586",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:29:42.118551Z",
     "start_time": "2023-01-28T13:29:42.115805Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required collections\n",
    "tx = db[\"transaction_network\"]\n",
    "tx_last_ind = db2[\"last_index_neo4j_stream\"]\n",
    "\n",
    "# import required temporary collections to overwrite with new data\n",
    "tx_tmp = db2[\"neo4j_stream_temporary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8485c39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:29:42.157895Z",
     "start_time": "2023-01-28T13:29:42.119927Z"
    }
   },
   "outputs": [],
   "source": [
    "#retrieve the last indices that were processed before\n",
    "tx_last_processed = tx_last_ind.find_one({'collection': 'neo4j_stream'})['last_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27456a7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:29:42.163894Z",
     "start_time": "2023-01-28T13:29:42.160599Z"
    }
   },
   "outputs": [],
   "source": [
    "# count how many documents are in each new input mongodb collection\n",
    "count_tx = tx.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "857c60f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:29:42.178140Z",
     "start_time": "2023-01-28T13:29:42.165237Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# for each Cardano table, select the records which haven't been processed yet (range between last_processed and total records count)\n",
    "tx_df = tx.find()[tx_last_processed:count_tx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d43d2ab8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:29:42.189544Z",
     "start_time": "2023-01-28T13:29:42.179367Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop the previous records in the temporary collections\n",
    "tx_tmp.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1d50ebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:24.303168Z",
     "start_time": "2023-01-28T13:29:42.190869Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f2019d91590>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the temporary records in the temporary collections\n",
    "tx_tmp.insert_many(tx_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dd4e46",
   "metadata": {},
   "source": [
    "### Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b28635a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:24.310508Z",
     "start_time": "2023-01-28T13:46:24.305440Z"
    }
   },
   "outputs": [],
   "source": [
    "config = pyspark.SparkConf().setAll([\n",
    "    ('spark.executor.memory', '30g'), \n",
    "    ('spark.executor.cores', '5'), # number of cores to use on each executor\n",
    "    ('spark.cores.max', '15'), # the maximum amount of CPU cores to request for the application from across the cluster\n",
    "    ('spark.driver.memory','20g'),\n",
    "    ('spark.driver.maxResultSize', '4g'),\n",
    "    ('spark.executor.instances', '3'),\n",
    "    ('spark.worker.cleanup.enabled', 'true'),\n",
    "    ('spark.worker.cleanup.interval', '60'),\n",
    "    ('spark.worker.cleanup.appDataTtl', '60'),\n",
    "    ('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector:10.0.2'),\n",
    "    ('spark.mongodb.output.writeConcern.wTimeoutMS','1000000'),\n",
    "    ('spark.mongodb.output.writeConcern.socketTimeoutMS','1000000'),\n",
    "    ('spark.mongodb.output.writeConcern.connectTimeoutMS','1000000'),\n",
    "    (\"neo4j.url\", \"bolt://172.23.149.210:7687\"),\n",
    "    (\"neo4j.authentication.type\", \"basic\"),\n",
    "    (\"neo4j.authentication.basic.username\", \"neo4j\"),\n",
    "    (\"neo4j.authentication.basic.password\", \"cardano\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d92ce16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:39.959806Z",
     "start_time": "2023-01-28T13:46:24.311916Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: neo4j.url\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.password\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.type\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.username\n",
      "23/01/28 14:46:30 WARN Utils: Your hostname, cardano-druid resolves to a loopback address: 127.0.0.1; using 172.23.149.210 instead (on interface ens3)\n",
      "23/01/28 14:46:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0fa4ac8d-b357-446b-a713-f1911de37cf5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector;10.0.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.5.1 in central\n",
      "\t[4.5.1] org.mongodb#mongodb-driver-sync;[4.5.0,4.5.99)\n",
      "\tfound org.mongodb#bson;4.5.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.5.1 in central\n",
      ":: resolution report :: resolve 2512ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.5.1 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector;10.0.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   1   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0fa4ac8d-b357-446b-a713-f1911de37cf5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/5ms)\n",
      "23/01/28 14:46:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/28 14:46:34 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf=config) \\\n",
    "    .appName(\"Neo4j-Stream-Update\") \\\n",
    "    .master(\"spark://172.23.149.210:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d83c4",
   "metadata": {},
   "source": [
    "#### Define schema for the different collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da62c525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:39.968518Z",
     "start_time": "2023-01-28T13:46:39.964687Z"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"address\", StringType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73629f56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:40.151639Z",
     "start_time": "2023-01-28T13:46:39.970214Z"
    }
   },
   "outputs": [],
   "source": [
    "schema2 = StructType([ \\\n",
    "    StructField(\"address\", StringType(), True), \\\n",
    "    StructField(\"id\", IntegerType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62953510",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:40.163047Z",
     "start_time": "2023-01-28T13:46:40.153538Z"
    }
   },
   "outputs": [],
   "source": [
    "schema3 = StructType([ \\\n",
    "    StructField(\"input_addr\", StringType(), True), \\\n",
    "    StructField(\"output_addr\", StringType(), True), \\\n",
    "    StructField(\"tx_hash\", StringType(), True), \\\n",
    "    StructField(\"input_ADA_value\", IntegerType(), True), \\\n",
    "    StructField(\"output_ADA_value\", IntegerType(), True), \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19a45d65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:40.173772Z",
     "start_time": "2023-01-28T13:46:40.164921Z"
    }
   },
   "outputs": [],
   "source": [
    "schema4 = StructType([ \\\n",
    "    StructField(\"hash\", StringType(), True), \\\n",
    "    StructField(\"id\", IntegerType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f3518",
   "metadata": {},
   "source": [
    "### Address Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d984a999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:42.325178Z",
     "start_time": "2023-01-28T13:46:40.175639Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'address_temporary') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85da919d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:42.724682Z",
     "start_time": "2023-01-28T13:46:42.326948Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_addresses.createOrReplaceTempView(\"temp_addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9aea741d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:42.797793Z",
     "start_time": "2023-01-28T13:46:42.733131Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_addresses2 = spark.sql(\"SELECT DISTINCT address FROM temp_addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd889087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:49.457290Z",
     "start_time": "2023-01-28T13:46:42.800086Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/28 14:46:42 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_addresses2.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'address_temporary_2') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f9cc1f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:49.480598Z",
     "start_time": "2023-01-28T13:46:49.460683Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'address_temporary_2') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59c4c70c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:49.497193Z",
     "start_time": "2023-01-28T13:46:49.482481Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_silver') \\\n",
    "  \t.option('spark.mongodb.collection', 'addresses') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34b9dfab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:49.533178Z",
     "start_time": "2023-01-28T13:46:49.498908Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses.createOrReplaceTempView(\"addresses\")\n",
    "temp_addresses.createOrReplaceTempView(\"temp_addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce9b625e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:46:49.806048Z",
     "start_time": "2023-01-28T13:46:49.535133Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "new_addresses = spark.sql(\"SELECT address FROM temp_addresses WHERE NOT EXISTS (SELECT address FROM addresses)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12a89f66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:20.937950Z",
     "start_time": "2023-01-28T13:46:49.807962Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/28 14:46:49 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_addresses.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'new_addresses_temporary') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47df216b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:20.944470Z",
     "start_time": "2023-01-28T13:48:20.940365Z"
    }
   },
   "outputs": [],
   "source": [
    "# find first address id for new addresses collection\n",
    "addr = db[\"addresses\"]\n",
    "count = addr.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76ba7542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:21.086275Z",
     "start_time": "2023-01-28T13:48:20.946105Z"
    }
   },
   "outputs": [],
   "source": [
    "# create ids for the new addresses\n",
    "collection = db2.new_addresses_temporary.find()\n",
    "for doc in collection:\n",
    "    update = {'$set': {\"id\": count}}\n",
    "    db2.new_addresses_temporary.update_one(doc, update)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa65bc90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:21.108158Z",
     "start_time": "2023-01-28T13:48:21.088711Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the new addresses with their ids\n",
    "final_new_addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'new_addresses_temporary') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema2) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76931619",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:21.391379Z",
     "start_time": "2023-01-28T13:48:21.110233Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/28 14:48:21 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "23/01/28 14:48:21 WARN Partitioner: Unable to get collection stats (collstats) returning a single partition.\n",
      "23/01/28 14:48:21 WARN Partitioner: Unable to get collection stats (collstats) returning a single partition.\n"
     ]
    }
   ],
   "source": [
    "# append new unique addresses to the existing collection of addresses\n",
    "final_new_addresses.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_silver') \\\n",
    "   .option('spark.mongodb.collection', 'addresses') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fd2c69b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:23.244721Z",
     "start_time": "2023-01-28T13:48:21.393416Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/28 14:48:21 WARN Partitioner: Unable to get collection stats (collstats) returning a single partition.\n",
      "23/01/28 14:48:21 WARN Partitioner: Unable to get collection stats (collstats) returning a single partition.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# stream new addresses to neo4j\n",
    "final_new_addresses.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"labels\", \"Address\") \\\n",
    "  .option(\"node.keys\", \"id\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7693a57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:23.267275Z",
     "start_time": "2023-01-28T13:48:23.249757Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop temporary collection\n",
    "db2.address_temporary_2.drop()\n",
    "db2.new_addresses_temporary.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1de8ef89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:23.362133Z",
     "start_time": "2023-01-28T13:48:23.268763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x7f210c1bdb10>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update checkpoints\n",
    "addr_query = {\"collection\": \"address\"}\n",
    "new_count = {\"$set\":{\"last_index\": count_addr}}\n",
    "addr_last_ind.update_one(addr_query, new_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc37ae",
   "metadata": {},
   "source": [
    "### Transaction Network & Clusters Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1289b3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:23.653273Z",
     "start_time": "2023-01-28T13:48:23.363863Z"
    }
   },
   "outputs": [],
   "source": [
    "TxNetwork = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_temporary') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema3) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54c9dc17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:23.753969Z",
     "start_time": "2023-01-28T13:48:23.655157Z"
    }
   },
   "outputs": [],
   "source": [
    "TxNetwork.createOrReplaceTempView(\"TxNetwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2e49fb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:23.954085Z",
     "start_time": "2023-01-28T13:48:23.755448Z"
    }
   },
   "outputs": [],
   "source": [
    "tx = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'node.public.tx') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema4) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9e5954e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:24.047164Z",
     "start_time": "2023-01-28T13:48:23.956008Z"
    }
   },
   "outputs": [],
   "source": [
    "tx.createOrReplaceTempView(\"tx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2b415fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:24.163462Z",
     "start_time": "2023-01-28T13:48:24.048614Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id = spark.sql(\"SELECT tx_hash, id as tx_id, input_addr, output_addr, input_ADA_value, output_ADA_value FROM TxNetwork LEFT JOIN tx on TxNetwork.tx_hash = tx.hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "193549ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:24.180012Z",
     "start_time": "2023-01-28T13:48:24.165498Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id.createOrReplaceTempView(\"transactions_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a3eea9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:24.203221Z",
     "start_time": "2023-01-28T13:48:24.181591Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_silver') \\\n",
    "  \t.option('spark.mongodb.collection', 'addresses') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema2) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c803aee8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:24.258449Z",
     "start_time": "2023-01-28T13:48:24.205001Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses.createOrReplaceTempView(\"addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37b62d66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:24.290493Z",
     "start_time": "2023-01-28T13:48:24.260199Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "input_addresses = spark.sql(\"SELECT transactions_id.tx_hash, transactions_id.tx_id, transactions_id.input_addr, transactions_id.output_addr, transactions_id.input_ADA_value, transactions_id.output_ADA_value, addresses.id as input_id FROM transactions_id LEFT JOIN addresses on transactions_id.input_addr = addresses.address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "104ecf02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:24.379531Z",
     "start_time": "2023-01-28T13:48:24.292428Z"
    }
   },
   "outputs": [],
   "source": [
    "input_addresses.createOrReplaceTempView(\"input_addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4759649d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T13:48:24.422192Z",
     "start_time": "2023-01-28T13:48:24.381289Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "TxNet = spark.sql(\"SELECT input_addresses.tx_hash, input_addresses.tx_id, input_addresses.input_addr, input_addresses.output_addr, input_addresses.input_ADA_value, input_addresses.output_ADA_value, input_addresses.input_id, addresses.id as output_id FROM input_addresses LEFT JOIN addresses on input_addresses.output_addr = addresses.address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e50718",
   "metadata": {},
   "source": [
    "### Neo4j Tx Network & Clusters Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a742f057",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T14:00:24.877411Z",
     "start_time": "2023-01-28T13:48:24.423904Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/28 14:52:45 WARN TaskSetManager: Lost task 70.0 in stage 7.0 (TID 181) (172.23.149.210 executor 2): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/01/28 14:52:45 WARN TaskSetManager: Lost task 75.0 in stage 7.0 (TID 186) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 37 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# stream transaction network\n",
    "TxNet.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.210:7687\") \\\n",
    "  .option(\"query\",\"MATCH (a1:Address {id: event.input_id}) MATCH (a2:Address {id: event.output_id}) CREATE (a1)-[:TRANSACTED_WITH {tx_hash: event.tx_hash, tx_id: event.tx_id, input_ADA_value: event.input_ADA_value, output_ADA_value: event.output_ADA_value}]->(a2)\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6dc04f0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T14:00:24.914685Z",
     "start_time": "2023-01-28T14:00:24.889077Z"
    }
   },
   "outputs": [],
   "source": [
    "TxNet.createOrReplaceTempView(\"TxNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1166ebd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T14:00:25.172580Z",
     "start_time": "2023-01-28T14:00:24.916732Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "clusters = spark.sql(\"SELECT t1.tx_id, t1.input_id as input1, t2.input_id as input2 FROM TxNet t1 LEFT JOIN TxNet t2 on t1.tx_id = t2.tx_id WHERE t1.input_id != t2.input_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc9d23c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T14:00:25.187914Z",
     "start_time": "2023-01-28T14:00:25.175388Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters.createOrReplaceTempView(\"clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "949d5524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T14:00:25.214663Z",
     "start_time": "2023-01-28T14:00:25.189801Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters_final = spark.sql(\"SELECT distinct tx_id, input1, input2 FROM clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a66407d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T14:07:00.030483Z",
     "start_time": "2023-01-28T14:00:25.216142Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/28 15:01:20 WARN TaskSetManager: Lost task 0.0 in stage 26.0 (TID 1003) (172.23.149.210 executor 2): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/01/28 15:01:20 WARN TaskSetManager: Lost task 1.0 in stage 25.0 (TID 989) (172.23.149.210 executor 2): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 37 more\n",
      "\n",
      "23/01/28 15:01:20 WARN TaskSetManager: Lost task 5.0 in stage 25.0 (TID 993) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:726)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/28 15:02:21 WARN TaskSetManager: Lost task 68.0 in stage 27.0 (TID 1534) (172.23.149.210 executor 2): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/01/28 15:02:21 WARN TaskSetManager: Lost task 61.0 in stage 27.0 (TID 1527) (172.23.149.210 executor 2): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 37 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/28 15:03:34 WARN TaskSetManager: Lost task 430.0 in stage 30.0 (TID 2107) (172.23.149.210 executor 2): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 37 more\n",
      "\n",
      "23/01/28 15:03:34 WARN TaskSetManager: Lost task 427.0 in stage 30.0 (TID 2104) (172.23.149.210 executor 2): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# stream clusering network\n",
    "clusters_final.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.210:7687\") \\\n",
    "  .option(\"query\",\"MATCH (a1:Address {id: event.input1}) MATCH (a2:Address {id: event.input2}) CREATE (a1)-[:ASSOCIATED_WITH]->(a2)\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "085ae9fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T14:07:00.050456Z",
     "start_time": "2023-01-28T14:07:00.035905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x7f210c1bb450>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update checkpoints\n",
    "tx_query = {\"collection\": \"neo4j_stream\"}\n",
    "new_count = {\"$set\":{\"last_index\": count_tx}}\n",
    "tx_last_ind.update_one(tx_query, new_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c54444e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T14:07:00.590629Z",
     "start_time": "2023-01-28T14:07:00.052037Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e952631a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,auto:percent",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
