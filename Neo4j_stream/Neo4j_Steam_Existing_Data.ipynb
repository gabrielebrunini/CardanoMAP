{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b974153-61cb-46e1-bb75-bad3ebb48163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T21:41:22.718743Z",
     "start_time": "2023-01-13T21:41:19.394588Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pyspark\n",
    "from pyspark.sql.types import  *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d549ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T21:41:24.896697Z",
     "start_time": "2023-01-13T21:41:24.609132Z"
    }
   },
   "outputs": [],
   "source": [
    "client = MongoClient(\"172.23.149.210\", 27017)\n",
    "db = client['cardano_silver']\n",
    "db2 = client['cardano_bronze']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d97b2cc4-2794-473d-9867-06b39d80e7b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T21:41:31.134648Z",
     "start_time": "2023-01-13T21:41:31.129584Z"
    }
   },
   "outputs": [],
   "source": [
    "config = pyspark.SparkConf().setAll([\n",
    "    ('spark.executor.memory', '90g'), \n",
    "    ('spark.executor.cores', '5'), # number of cores to use on each executor\n",
    "    ('spark.cores.max', '5'), # the maximum amount of CPU cores to request for the application from across the cluster\n",
    "    ('spark.driver.memory','20g'),\n",
    "    ('spark.executor.instances', '1'),\n",
    "    ('spark.worker.cleanup.enabled', 'true'),\n",
    "    ('spark.worker.cleanup.interval', '60'),\n",
    "    ('spark.worker.cleanup.appDataTtl', '60'),\n",
    "    ('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector:10.0.2'),\n",
    "    ('spark.mongodb.output.writeConcern.wTimeoutMS','1000000'),\n",
    "    ('spark.mongodb.output.writeConcern.socketTimeoutMS','1000000'),\n",
    "    ('spark.mongodb.output.writeConcern.connectTimeoutMS','1000000'),\n",
    "    (\"neo4j.url\", \"bolt://172.23.149.210:7687\"),\n",
    "    (\"neo4j.authentication.type\", \"basic\"),\n",
    "    (\"neo4j.authentication.basic.username\", \"neo4j\"),\n",
    "    (\"neo4j.authentication.basic.password\", \"cardano\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3834539a-c8a6-45d5-bc38-ded804d54e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T21:41:49.983183Z",
     "start_time": "2023-01-13T21:41:33.643277Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: neo4j.url\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.password\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.type\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.username\n",
      "23/01/13 22:41:40 WARN Utils: Your hostname, cardano-druid resolves to a loopback address: 127.0.0.1; using 172.23.149.210 instead (on interface ens3)\n",
      "23/01/13 22:41:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e2b45ca8-2254-480f-a052-b8e8656d26e2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector;10.0.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.5.1 in central\n",
      "\t[4.5.1] org.mongodb#mongodb-driver-sync;[4.5.0,4.5.99)\n",
      "\tfound org.mongodb#bson;4.5.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.5.1 in central\n",
      ":: resolution report :: resolve 2983ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.5.1 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector;10.0.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   1   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e2b45ca8-2254-480f-a052-b8e8656d26e2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/5ms)\n",
      "23/01/13 22:41:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/13 22:41:44 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "\t.config(conf=config) \\\n",
    "    .appName(\"Neo4j-Stream\") \\\n",
    "    .master(\"spark://172.23.149.210:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0c91b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T21:41:49.989444Z",
     "start_time": "2023-01-13T21:41:49.986703Z"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"address\", StringType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a635e71-6f23-4c4e-aea1-68bb66069475",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T21:30:13.553734Z",
     "start_time": "2023-01-08T21:30:13.231111Z"
    }
   },
   "outputs": [],
   "source": [
    "schema2 = StructType([ \\\n",
    "    StructField(\"input_addr\", StringType(), True), \\\n",
    "    StructField(\"output_addr\", StringType(), True), \\\n",
    "    StructField(\"tx_hash\", StringType(), True), \\\n",
    "    StructField(\"input_ADA_value\", IntegerType(), True), \\\n",
    "    StructField(\"output_ADA_value\", IntegerType(), True), \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2490517c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T21:42:46.909978Z",
     "start_time": "2023-01-13T21:42:46.907155Z"
    }
   },
   "outputs": [],
   "source": [
    "schema3 = StructType([ \\\n",
    "    StructField(\"address\", StringType(), True), \\\n",
    "    StructField(\"id\", IntegerType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4393d5bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T16:29:47.910201Z",
     "start_time": "2023-01-11T16:29:47.836112Z"
    }
   },
   "outputs": [],
   "source": [
    "schema4 = StructType([ \\\n",
    "    StructField(\"input_addr\", StringType(), True), \\\n",
    "    StructField(\"output_addr\", StringType(), True), \\\n",
    "    StructField(\"tx_hash\", StringType(), True), \\\n",
    "    StructField(\"tx_id\", IntegerType(), True), \\\n",
    "    StructField(\"input_ADA_value\", IntegerType(), True), \\\n",
    "    StructField(\"output_ADA_value\", IntegerType(), True), \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5030b726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T21:30:15.848598Z",
     "start_time": "2023-01-08T21:30:15.102099Z"
    }
   },
   "outputs": [],
   "source": [
    "schema5 = StructType([ \\\n",
    "    StructField(\"hash\", StringType(), True), \\\n",
    "    StructField(\"id\", IntegerType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72bf191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T22:15:34.526233Z",
     "start_time": "2023-01-13T22:15:34.520572Z"
    }
   },
   "outputs": [],
   "source": [
    "schema6 = StructType([ \\\n",
    "    StructField(\"input_addr\", StringType(), True), \\\n",
    "    StructField(\"output_addr\", StringType(), True), \\\n",
    "    StructField(\"tx_hash\", StringType(), True), \\\n",
    "    StructField(\"tx_id\", IntegerType(), True), \\\n",
    "    StructField(\"input_ADA_value\", IntegerType(), True), \\\n",
    "    StructField(\"output_ADA_value\", IntegerType(), True), \\\n",
    "    StructField(\"input_id\", IntegerType(), True), \\\n",
    "    StructField(\"output_id\", IntegerType(), True), \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d8ee79c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-24T11:54:19.527587Z",
     "start_time": "2022-12-24T11:54:17.972718Z"
    }
   },
   "outputs": [],
   "source": [
    "tx_out = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'node.public.tx_out') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c5988d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-24T11:54:19.936006Z",
     "start_time": "2022-12-24T11:54:19.530010Z"
    }
   },
   "outputs": [],
   "source": [
    "tx_out.createOrReplaceTempView(\"tx_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "446848b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-24T11:54:20.006980Z",
     "start_time": "2022-12-24T11:54:19.938333Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses_raw = spark.sql(\"SELECT DISTINCT address FROM tx_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "713a7667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-24T12:06:22.228020Z",
     "start_time": "2022-12-24T11:54:20.009560Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/24 12:54:20 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "22/12/24 12:54:36 WARN TaskSetManager: Lost task 21.0 in stage 0.0 (TID 21) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/12/24 12:54:36 WARN TaskSetManager: Lost task 16.0 in stage 0.0 (TID 16) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 38 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/24 12:56:06 WARN TaskSetManager: Lost task 478.0 in stage 0.0 (TID 483) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/12/24 12:57:49 WARN TaskSetManager: Lost task 936.0 in stage 0.0 (TID 946) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "addresses_raw.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'addresses') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca6e540c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T12:49:27.729038Z",
     "start_time": "2022-12-23T12:39:18.197779Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "addresses_raw.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"labels\", \"Address\") \\\n",
    "  .option(\"node.keys\", \"address\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create id for each address in neo4j using internal ids using:\n",
    "# CALL apoc.periodic.iterate(\"MATCH (n:Address) RETURN n\",\"SET n.id = id(n)\",{batchSize:10000, parallel:false})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da73e6c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-24T12:13:42.511513Z",
     "start_time": "2022-12-24T12:13:39.819066Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses_id = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"labels\", \"Address\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "133f6e44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-24T12:13:45.379304Z",
     "start_time": "2022-12-24T12:13:45.366393Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses_id.createOrReplaceTempView(\"addresses_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "736b591a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-24T12:13:53.168551Z",
     "start_time": "2022-12-24T12:13:53.155636Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses_id = spark.sql(\"SELECT id, address FROM addresses_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e963054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-24T12:56:29.373821Z",
     "start_time": "2022-12-24T12:13:56.060794Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/24 13:13:56 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "addresses_id.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_silver') \\\n",
    "   .option('spark.mongodb.collection', 'addresses') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1a03a",
   "metadata": {},
   "source": [
    "### Transaction Network Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9137f7ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T13:30:32.419421Z",
     "start_time": "2023-01-07T13:30:30.108712Z"
    }
   },
   "outputs": [],
   "source": [
    "# read only the tx id and hash\n",
    "tx = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'node.public.tx') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema5) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95b0f71b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T13:45:14.457432Z",
     "start_time": "2023-01-07T13:30:34.016875Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/07 14:30:34 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# store the tx id and hash\n",
    "tx.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'tx_ids') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59647d48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T19:53:37.908449Z",
     "start_time": "2023-01-07T19:53:37.883504Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the stored collection\n",
    "tx = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'tx_ids') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema5) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41dfbdda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T19:53:39.250965Z",
     "start_time": "2023-01-07T19:53:39.231579Z"
    }
   },
   "outputs": [],
   "source": [
    "tx.createOrReplaceTempView(\"tx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNetwork = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_silver') \\\n",
    "  \t.option('spark.mongodb.collection', 'transaction_network') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema2) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNetwork.createOrReplaceTempView(\"TxNetwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f87a2e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T19:53:42.246260Z",
     "start_time": "2023-01-07T19:53:42.124102Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# join the transaction network to tx in order to add tx_id to the transaction network collection\n",
    "transactions_id = spark.sql(\"SELECT tx_hash, id as tx_id, input_addr, output_addr, input_ADA_value, output_ADA_value FROM TxNetwork LEFT JOIN tx on TxNetwork.tx_hash = tx.hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4b73026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T22:53:26.762759Z",
     "start_time": "2023-01-07T19:53:42.567318Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/07 20:53:42 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "23/01/07 21:20:16 WARN TaskSetManager: Lost task 2241.0 in stage 0.0 (TID 2241) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/01/07 21:20:16 WARN TaskSetManager: Lost task 2243.0 in stage 0.0 (TID 2243) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 37 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/07 21:23:57 WARN TaskSetManager: Lost task 74.0 in stage 1.0 (TID 2798) (172.23.149.210 executor 0): com.mongodb.MongoConnectionPoolClearedException: Connection pool for ServerId{clusterId=ClusterId{value='63b9d159acbfb6168c7e4a6b', description='null'}, address=172.23.149.210:27017} is paused because another operation failed\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$StateAndGeneration.throwIfClosedOrPaused(DefaultConnectionPool.java:1533)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool.get(DefaultConnectionPool.java:174)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool.get(DefaultConnectionPool.java:166)\n",
      "\tat com.mongodb.internal.connection.DefaultServer.getConnection(DefaultServer.java:103)\n",
      "\tat com.mongodb.internal.binding.ClusterBinding$ClusterBindingConnectionSource.getConnection(ClusterBinding.java:175)\n",
      "\tat com.mongodb.client.internal.ClientSessionBinding$SessionBindingConnectionSource.getConnection(ClientSessionBinding.java:192)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.connection(QueryBatchCursor.java:540)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:522)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: com.mongodb.MongoSocketReadTimeoutException: Timeout while receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:701)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receive(InternalStreamConnection.java:374)\n",
      "\tat com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:216)\n",
      "\tat com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:152)\n",
      "\t... 1 more\n",
      "Caused by: java.net.SocketTimeoutException: Read timed out\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 5 more\n",
      "\n",
      "23/01/07 21:23:58 WARN TaskSetManager: Lost task 73.0 in stage 1.0 (TID 2797) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/07 22:30:13 WARN TaskSetManager: Lost task 91.0 in stage 4.0 (TID 2916) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:477)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:339)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:260)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:575)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:574)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:573)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:232)\n",
      "\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:268)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:84)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:443)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:423)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.writeModels(MongoDataWriter.java:200)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:93)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: PartitionId: 91, TaskId: 2916. Manual data clean up may be required.\n",
      "\t\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.abort(MongoDataWriter.java:121)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:453)\n",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\t\t... 10 more\n",
      "\n",
      "23/01/07 22:30:13 WARN TaskSetManager: Lost task 90.0 in stage 4.0 (TID 2915) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:477)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:339)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:260)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:575)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:574)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:573)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:232)\n",
      "\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:268)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:84)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:443)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:423)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.writeModels(MongoDataWriter.java:200)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:93)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: PartitionId: 90, TaskId: 2915. Manual data clean up may be required.\n",
      "\t\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.abort(MongoDataWriter.java:121)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:453)\n",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\t\t... 10 more\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 39 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/07 23:29:45 WARN TaskSetManager: Lost task 174.0 in stage 4.0 (TID 3004) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:477)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:339)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:260)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:575)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:574)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:573)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:232)\n",
      "\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:268)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:84)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:443)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:423)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.writeModels(MongoDataWriter.java:200)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:93)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: PartitionId: 174, TaskId: 3004. Manual data clean up may be required.\n",
      "\t\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.abort(MongoDataWriter.java:121)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:453)\n",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\t\t... 10 more\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 39 more\n",
      "\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions_id.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'neo4j_stream_stage1') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac73f92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T14:38:31.043760Z",
     "start_time": "2023-01-10T14:38:29.351709Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage1') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema4) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b16c958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T14:38:32.262887Z",
     "start_time": "2023-01-10T14:38:32.072899Z"
    }
   },
   "outputs": [],
   "source": [
    "# divide the transactions to avoid memory or storage related issues\n",
    "transactions_id_1 = transactions_id.filter(col(\"tx_id\").between(0,12000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32749baf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T15:49:07.454694Z",
     "start_time": "2023-01-10T14:38:33.794910Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/10 15:38:34 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions_id_1.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'neo4j_stream_stage2_1') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "750316cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T15:50:34.991659Z",
     "start_time": "2023-01-10T15:50:32.897339Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_silver') \\\n",
    "  \t.option('spark.mongodb.collection', 'addresses') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema3) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a02f134d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T15:50:35.478375Z",
     "start_time": "2023-01-10T15:50:34.993849Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses.createOrReplaceTempView(\"addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89e71cee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T15:50:37.246003Z",
     "start_time": "2023-01-10T15:50:35.480881Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id_1 = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage2_1') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema4) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22389c34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T15:50:37.267552Z",
     "start_time": "2023-01-10T15:50:37.247935Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id_1.createOrReplaceTempView(\"transactions_id_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a28de72f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T15:50:37.389633Z",
     "start_time": "2023-01-10T15:50:37.270252Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# join transaction network to addresses to get the input address ids\n",
    "input_addresses = spark.sql(\"SELECT transactions_id_1.tx_hash, transactions_id_1.tx_id, transactions_id_1.input_addr, transactions_id_1.output_addr, transactions_id_1.input_ADA_value, transactions_id_1.output_ADA_value, addresses.id as input_id FROM transactions_id_1 LEFT JOIN addresses on transactions_id_1.input_addr = addresses.address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d86d658f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T15:50:37.418131Z",
     "start_time": "2023-01-10T15:50:37.391581Z"
    }
   },
   "outputs": [],
   "source": [
    "input_addresses.createOrReplaceTempView(\"input_addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e4704ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T15:50:38.223162Z",
     "start_time": "2023-01-10T15:50:38.183383Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# join the previous dataframe to addresses to get the output ids\n",
    "TxNet = spark.sql(\"SELECT input_addresses.tx_hash, input_addresses.tx_id, input_addresses.input_addr, input_addresses.output_addr, input_addresses.input_ADA_value, input_addresses.output_ADA_value, input_addresses.input_id, addresses.id as output_id FROM input_addresses LEFT JOIN addresses on input_addresses.output_addr = addresses.address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7723f22c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T16:49:42.274727Z",
     "start_time": "2023-01-10T15:50:41.508995Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/10 16:50:41 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "23/01/10 17:09:43 WARN TaskSetManager: Lost task 542.0 in stage 0.0 (TID 542) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 37 more\n",
      "\n",
      "23/01/10 17:09:44 WARN TaskSetManager: Lost task 541.0 in stage 0.0 (TID 541) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# store the result, which includes the original transaction network with columns for tx_id, input addr. id, \n",
    "# and output addr. id to make neo4j queries more effecient\n",
    "TxNet.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'neo4j_stream_stage3_1') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c35ae3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T16:54:11.909247Z",
     "start_time": "2023-01-10T16:54:09.718665Z"
    }
   },
   "outputs": [],
   "source": [
    "# repeat the previous process with a new part of the original data\n",
    "transactions_id = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage1') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema4) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c2e4af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T16:54:13.151667Z",
     "start_time": "2023-01-10T16:54:12.936318Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id_2 = transactions_id.filter(col(\"tx_id\").between(12000001,28000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd66555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T18:03:48.297695Z",
     "start_time": "2023-01-10T16:54:20.966272Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/10 17:54:21 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions_id_2.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'neo4j_stream_stage2_2') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "446d848c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T18:08:18.707604Z",
     "start_time": "2023-01-10T18:08:16.268445Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_silver') \\\n",
    "  \t.option('spark.mongodb.collection', 'addresses') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema3) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6ec567",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T18:08:19.258584Z",
     "start_time": "2023-01-10T18:08:18.709410Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses.createOrReplaceTempView(\"addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d4937b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T18:08:19.288028Z",
     "start_time": "2023-01-10T18:08:19.260961Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id_2 = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage2_2') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema4) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d0bac5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T18:08:19.309285Z",
     "start_time": "2023-01-10T18:08:19.289656Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id_2.createOrReplaceTempView(\"transactions_id_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b760de03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T18:08:19.422159Z",
     "start_time": "2023-01-10T18:08:19.310986Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "input_addresses = spark.sql(\"SELECT transactions_id_2.tx_hash, transactions_id_2.tx_id, transactions_id_2.input_addr, transactions_id_2.output_addr, transactions_id_2.input_ADA_value, transactions_id_2.output_ADA_value, addresses.id as input_id FROM transactions_id_2 LEFT JOIN addresses on transactions_id_2.input_addr = addresses.address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba2d5f23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T18:08:19.457801Z",
     "start_time": "2023-01-10T18:08:19.429857Z"
    }
   },
   "outputs": [],
   "source": [
    "input_addresses.createOrReplaceTempView(\"input_addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bdb3a2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T18:08:19.501951Z",
     "start_time": "2023-01-10T18:08:19.459227Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "TxNet = spark.sql(\"SELECT input_addresses.tx_hash, input_addresses.tx_id, input_addresses.input_addr, input_addresses.output_addr, input_addresses.input_ADA_value, input_addresses.output_ADA_value, input_addresses.input_id, addresses.id as output_id FROM input_addresses LEFT JOIN addresses on input_addresses.output_addr = addresses.address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6c4198a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T19:23:57.175731Z",
     "start_time": "2023-01-10T18:08:19.503400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/10 19:08:19 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "23/01/10 19:30:23 WARN TaskSetManager: Lost task 88.0 in stage 1.0 (TID 655) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 37 more\n",
      "\n",
      "23/01/10 19:40:35 WARN TaskSetManager: Lost task 26.0 in stage 10.0 (TID 982) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:477)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:339)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:260)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:575)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:574)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:573)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:232)\n",
      "\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:268)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:84)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:443)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:423)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.writeModels(MongoDataWriter.java:200)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:93)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: PartitionId: 26, TaskId: 982. Manual data clean up may be required.\n",
      "\t\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.abort(MongoDataWriter.java:121)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:453)\n",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\t\t... 10 more\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 39 more\n",
      "\n",
      "23/01/10 19:40:35 WARN TaskSetManager: Lost task 20.0 in stage 10.0 (TID 976) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:477)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:339)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:260)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:575)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:574)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:573)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:232)\n",
      "\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:268)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:84)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:443)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:423)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.writeModels(MongoDataWriter.java:200)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:93)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: PartitionId: 20, TaskId: 976. Manual data clean up may be required.\n",
      "\t\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.abort(MongoDataWriter.java:121)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:453)\n",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\t\t... 10 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/10 19:47:47 WARN TaskSetManager: Lost task 40.0 in stage 10.0 (TID 1001) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:477)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:339)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:260)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:575)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:574)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:573)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:232)\n",
      "\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:268)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:84)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:443)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:423)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.writeModels(MongoDataWriter.java:200)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:93)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: PartitionId: 40, TaskId: 1001. Manual data clean up may be required.\n",
      "\t\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.abort(MongoDataWriter.java:121)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:453)\n",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\t\t... 10 more\n",
      "\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "TxNet.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'neo4j_stream_stage3_2') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebbadd92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T19:36:51.024575Z",
     "start_time": "2023-01-10T19:36:49.095089Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage1') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema4) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a4212e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T19:36:51.227861Z",
     "start_time": "2023-01-10T19:36:51.027373Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id_3 = transactions_id.filter(col(\"tx_id\").between(28000001,34000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f0384d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T21:09:22.099814Z",
     "start_time": "2023-01-10T19:36:53.813155Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/10 20:36:54 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions_id_3.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'neo4j_stream_stage2_3') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f4ce6f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T21:17:17.933869Z",
     "start_time": "2023-01-10T21:17:14.679015Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_silver') \\\n",
    "  \t.option('spark.mongodb.collection', 'addresses') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema3) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109643cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T21:17:18.378616Z",
     "start_time": "2023-01-10T21:17:17.935516Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses.createOrReplaceTempView(\"addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a71865a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T21:17:21.311176Z",
     "start_time": "2023-01-10T21:17:21.284593Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id_3 = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage2_3') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema4) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a26ea839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T21:17:25.081025Z",
     "start_time": "2023-01-10T21:17:25.063025Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id_3.createOrReplaceTempView(\"transactions_id_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa49c581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T21:17:46.145593Z",
     "start_time": "2023-01-10T21:17:46.049929Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "input_addresses = spark.sql(\"SELECT transactions_id_3.tx_hash, transactions_id_3.tx_id, transactions_id_3.input_addr, transactions_id_3.output_addr, transactions_id_3.input_ADA_value, transactions_id_3.output_ADA_value, addresses.id as input_id FROM transactions_id_3 LEFT JOIN addresses on transactions_id_3.input_addr = addresses.address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aa6ca17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T21:17:51.069533Z",
     "start_time": "2023-01-10T21:17:51.046755Z"
    }
   },
   "outputs": [],
   "source": [
    "input_addresses.createOrReplaceTempView(\"input_addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c27150a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T21:18:00.837577Z",
     "start_time": "2023-01-10T21:18:00.798089Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "TxNet = spark.sql(\"SELECT input_addresses.tx_hash, input_addresses.tx_id, input_addresses.input_addr, input_addresses.output_addr, input_addresses.input_ADA_value, input_addresses.output_ADA_value, input_addresses.input_id, addresses.id as output_id FROM input_addresses LEFT JOIN addresses on input_addresses.output_addr = addresses.address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "166ac84e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T22:34:05.096420Z",
     "start_time": "2023-01-10T21:18:05.471904Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/10 22:18:05 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "TxNet.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'neo4j_stream_stage3_3') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6357fd8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T23:46:20.775857Z",
     "start_time": "2023-01-10T23:46:15.699678Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage1') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema4) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e56545a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T23:46:21.003266Z",
     "start_time": "2023-01-10T23:46:20.777305Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id_4 = transactions_id.filter(col(\"tx_id\").between(34000001,40000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3417a98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T00:55:22.513768Z",
     "start_time": "2023-01-10T23:46:21.004733Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/11 00:46:21 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions_id_4.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'neo4j_stream_stage2_4') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba735c9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T01:03:43.606400Z",
     "start_time": "2023-01-11T01:03:39.600622Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_silver') \\\n",
    "  \t.option('spark.mongodb.collection', 'addresses') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema3) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52fe4ce8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T01:03:44.765656Z",
     "start_time": "2023-01-11T01:03:43.608789Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses.createOrReplaceTempView(\"addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f226a90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T01:03:44.802347Z",
     "start_time": "2023-01-11T01:03:44.767893Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id_4 = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage2_4') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema4) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8db04072",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T01:03:44.861398Z",
     "start_time": "2023-01-11T01:03:44.803557Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id_4.createOrReplaceTempView(\"transactions_id_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f3a33b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T01:03:45.107688Z",
     "start_time": "2023-01-11T01:03:44.862958Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "input_addresses = spark.sql(\"SELECT transactions_id_4.tx_hash, transactions_id_4.tx_id, transactions_id_4.input_addr, transactions_id_4.output_addr, transactions_id_4.input_ADA_value, transactions_id_4.output_ADA_value, addresses.id as input_id FROM transactions_id_4 LEFT JOIN addresses on transactions_id_4.input_addr = addresses.address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e767c4fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T01:03:45.129537Z",
     "start_time": "2023-01-11T01:03:45.108888Z"
    }
   },
   "outputs": [],
   "source": [
    "input_addresses.createOrReplaceTempView(\"input_addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47b35f34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T01:03:45.258758Z",
     "start_time": "2023-01-11T01:03:45.131148Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "TxNet = spark.sql(\"SELECT input_addresses.tx_hash, input_addresses.tx_id, input_addresses.input_addr, input_addresses.output_addr, input_addresses.input_ADA_value, input_addresses.output_ADA_value, input_addresses.input_id, addresses.id as output_id FROM input_addresses LEFT JOIN addresses on input_addresses.output_addr = addresses.address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f014fbde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T03:08:21.933756Z",
     "start_time": "2023-01-11T01:03:45.260017Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/11 02:03:45 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "23/01/11 02:32:01 WARN TaskSetManager: Lost task 38.0 in stage 2.0 (TID 567) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 37 more\n",
      "\n",
      "23/01/11 03:08:23 WARN TaskSetManager: Lost task 105.0 in stage 10.0 (TID 931) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:477)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:339)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:260)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:575)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:574)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:573)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:232)\n",
      "\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:268)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:84)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:443)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:423)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.writeModels(MongoDataWriter.java:200)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:93)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: PartitionId: 105, TaskId: 931. Manual data clean up may be required.\n",
      "\t\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.abort(MongoDataWriter.java:121)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:453)\n",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\t\t... 10 more\n",
      "\n",
      "23/01/11 03:08:23 WARN TaskSetManager: Lost task 90.0 in stage 10.0 (TID 916) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:477)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:339)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:260)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:575)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:574)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:573)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:232)\n",
      "\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:268)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:84)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:443)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:423)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.writeModels(MongoDataWriter.java:200)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:93)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: PartitionId: 90, TaskId: 916. Manual data clean up may be required.\n",
      "\t\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.abort(MongoDataWriter.java:121)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:453)\n",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\t\t... 10 more\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 39 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/11 03:31:30 WARN TaskSetManager: Lost task 135.0 in stage 10.0 (TID 966) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:477)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:339)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:260)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:575)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:574)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:573)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:232)\n",
      "\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:268)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:84)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:443)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:423)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.writeModels(MongoDataWriter.java:200)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:93)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: PartitionId: 135, TaskId: 966. Manual data clean up may be required.\n",
      "\t\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.abort(MongoDataWriter.java:121)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:453)\n",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\t\t... 10 more\n",
      "\n",
      "23/01/11 03:43:46 WARN TaskSetManager: Lost task 160.0 in stage 10.0 (TID 996) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:477)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:339)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:260)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:575)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:574)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:573)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:232)\n",
      "\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:268)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:84)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:443)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:423)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.writeModels(MongoDataWriter.java:200)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:93)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: PartitionId: 160, TaskId: 996. Manual data clean up may be required.\n",
      "\t\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.abort(MongoDataWriter.java:121)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:453)\n",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\t\t... 10 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/11 03:59:39 WARN TaskSetManager: Lost task 186.0 in stage 10.0 (TID 1026) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:477)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:339)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:260)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:575)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:574)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:573)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:232)\n",
      "\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:268)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:84)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:443)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:423)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.writeModels(MongoDataWriter.java:200)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:93)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: PartitionId: 186, TaskId: 1026. Manual data clean up may be required.\n",
      "\t\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.abort(MongoDataWriter.java:121)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:453)\n",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\t\t... 10 more\n",
      "\n",
      "23/01/11 03:59:39 WARN TaskSetManager: Lost task 185.0 in stage 10.0 (TID 1025) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:477)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:339)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:260)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:575)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:574)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:573)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:232)\n",
      "\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:268)\n",
      "\tat com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:84)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:443)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:423)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.writeModels(MongoDataWriter.java:200)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:93)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: PartitionId: 185, TaskId: 1025. Manual data clean up may be required.\n",
      "\t\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.abort(MongoDataWriter.java:121)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$6(WriteToDataSourceV2Exec.scala:453)\n",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1507)\n",
      "\t\t... 10 more\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 39 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "TxNet.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'neo4j_stream_stage3_4') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c39612",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T13:11:05.573331Z",
     "start_time": "2023-01-11T13:11:01.873578Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage1') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema4) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37689fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T13:11:05.784875Z",
     "start_time": "2023-01-11T13:11:05.574874Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id_5 = transactions_id.filter(col(\"tx_id\").between(40000001,58400000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78be0c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T14:29:16.952613Z",
     "start_time": "2023-01-11T13:11:05.787156Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/11 14:11:06 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions_id_5.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'neo4j_stream_stage2_5') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47578fd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T16:29:56.264030Z",
     "start_time": "2023-01-11T16:29:54.451104Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_silver') \\\n",
    "  \t.option('spark.mongodb.collection', 'addresses') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema3) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ee03074",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T16:29:56.834683Z",
     "start_time": "2023-01-11T16:29:56.267471Z"
    }
   },
   "outputs": [],
   "source": [
    "addresses.createOrReplaceTempView(\"addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8150239f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T16:29:56.862464Z",
     "start_time": "2023-01-11T16:29:56.836509Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id_5 = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage2_5') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema4) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e8dcd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T16:29:56.917809Z",
     "start_time": "2023-01-11T16:29:56.901839Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions_id_5.createOrReplaceTempView(\"transactions_id_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b343c450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T16:29:57.822428Z",
     "start_time": "2023-01-11T16:29:57.669335Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "input_addresses = spark.sql(\"SELECT transactions_id_5.tx_hash, transactions_id_5.tx_id, transactions_id_5.input_addr, transactions_id_5.output_addr, transactions_id_5.input_ADA_value, transactions_id_5.output_ADA_value, addresses.id as input_id FROM transactions_id_5 LEFT JOIN addresses on transactions_id_5.input_addr = addresses.address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "addfef85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T16:29:58.147484Z",
     "start_time": "2023-01-11T16:29:58.125109Z"
    }
   },
   "outputs": [],
   "source": [
    "input_addresses.createOrReplaceTempView(\"input_addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c7b57f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T16:29:58.979919Z",
     "start_time": "2023-01-11T16:29:58.947621Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "TxNet = spark.sql(\"SELECT input_addresses.tx_hash, input_addresses.tx_id, input_addresses.input_addr, input_addresses.output_addr, input_addresses.input_ADA_value, input_addresses.output_ADA_value, input_addresses.input_id, addresses.id as output_id FROM input_addresses LEFT JOIN addresses on input_addresses.output_addr = addresses.address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b99321a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-11T17:37:00.561231Z",
     "start_time": "2023-01-11T16:29:59.873445Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/11 17:30:00 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "23/01/11 17:50:16 WARN TaskSetManager: Lost task 351.0 in stage 0.0 (TID 351) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 37 more\n",
      "\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "TxNet.write.format(\"mongodb\") \\\n",
    "   .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"append\") \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'neo4j_stream_stage3_5') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff198d",
   "metadata": {},
   "source": [
    "## Neo4j Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a1772",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b4edb1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T22:15:42.546761Z",
     "start_time": "2023-01-13T22:15:42.509243Z"
    }
   },
   "outputs": [],
   "source": [
    "TxNet = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage3_1') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema6) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNet.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.210:7687\") \\\n",
    "  .option(\"query\",\"MATCH (a1:Address {id: event.input_id}) MATCH (a2:Address {id: event.output_id}) CREATE (a1)-[:TRANSACTED_WITH {tx_hash: event.tx_hash, tx_id: event.tx_id, input_ADA_value: event.input_ADA_value, output_ADA_value: event.output_ADA_value}]->(a2)\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "970a26d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T17:43:42.926725Z",
     "start_time": "2023-01-12T17:43:42.582994Z"
    }
   },
   "outputs": [],
   "source": [
    "TxNet.createOrReplaceTempView(\"TxNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a2cba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T17:43:45.332815Z",
     "start_time": "2023-01-12T17:43:45.217212Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "clusters = spark.sql(\"SELECT t1.tx_id, t1.input_id as input1, t2.input_id as input2 FROM TxNet t1 LEFT JOIN TxNet t2 on t1.tx_id = t2.tx_id WHERE t1.input_id != t2.input_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "879e8d6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T17:43:45.875189Z",
     "start_time": "2023-01-12T17:43:45.851232Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters.createOrReplaceTempView(\"clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad04cffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T17:43:46.884945Z",
     "start_time": "2023-01-12T17:43:46.861149Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters_final = spark.sql(\"SELECT distinct tx_id, input1, input2 FROM clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b23f3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-12T17:43:48.419Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/12 18:46:20 WARN TaskSetManager: Lost task 434.0 in stage 0.0 (TID 434) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/01/12 18:46:20 WARN TaskSetManager: Lost task 431.0 in stage 0.0 (TID 431) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 37 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/12 18:50:22 WARN TaskSetManager: Lost task 417.0 in stage 1.0 (TID 1014) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "[Stage 4:>                                                         (0 + 5) / 29]\r"
     ]
    }
   ],
   "source": [
    "clusters_final.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.210:7687\") \\\n",
    "  .option(\"query\",\"MATCH (a1:Address {id: event.input1}) MATCH (a2:Address {id: event.input2}) CREATE (a1)-[:ASSOCIATED_WITH]->(a2)\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645b9d4",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNet = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage3_2') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema6) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54fe285",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNet.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.210:7687\") \\\n",
    "  .option(\"query\",\"MATCH (a1:Address {id: event.input_id}) MATCH (a2:Address {id: event.output_id}) CREATE (a1)-[:TRANSACTED_WITH {tx_hash: event.tx_hash, tx_id: event.tx_id, input_ADA_value: event.input_ADA_value, output_ADA_value: event.output_ADA_value}]->(a2)\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1126f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNet.createOrReplaceTempView(\"TxNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = spark.sql(\"SELECT t1.tx_id, t1.input_id as input1, t2.input_id as input2 FROM TxNet t1 LEFT JOIN TxNet t2 on t1.tx_id = t2.tx_id WHERE t1.input_id != t2.input_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ae94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.createOrReplaceTempView(\"clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_final = spark.sql(\"SELECT distinct tx_id, input1, input2 FROM clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_final.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.210:7687\") \\\n",
    "  .option(\"query\",\"MATCH (a1:Address {id: event.input1}) MATCH (a2:Address {id: event.input2}) CREATE (a1)-[:ASSOCIATED_WITH]->(a2)\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4a726",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9774f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNet = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage3_3') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema6) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149adf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNet.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.210:7687\") \\\n",
    "  .option(\"query\",\"MATCH (a1:Address {id: event.input_id}) MATCH (a2:Address {id: event.output_id}) CREATE (a1)-[:TRANSACTED_WITH {tx_hash: event.tx_hash, tx_id: event.tx_id, input_ADA_value: event.input_ADA_value, output_ADA_value: event.output_ADA_value}]->(a2)\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNet.createOrReplaceTempView(\"TxNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = spark.sql(\"SELECT t1.tx_id, t1.input_id as input1, t2.input_id as input2 FROM TxNet t1 LEFT JOIN TxNet t2 on t1.tx_id = t2.tx_id WHERE t1.input_id != t2.input_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a460b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.createOrReplaceTempView(\"clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_final = spark.sql(\"SELECT distinct tx_id, input1, input2 FROM clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bccdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_final.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.210:7687\") \\\n",
    "  .option(\"query\",\"MATCH (a1:Address {id: event.input1}) MATCH (a2:Address {id: event.input2}) CREATE (a1)-[:ASSOCIATED_WITH]->(a2)\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5093c98",
   "metadata": {},
   "source": [
    "### Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNet = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage3_4') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema6) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11785bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNet.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.210:7687\") \\\n",
    "  .option(\"query\",\"MATCH (a1:Address {id: event.input_id}) MATCH (a2:Address {id: event.output_id}) CREATE (a1)-[:TRANSACTED_WITH {tx_hash: event.tx_hash, tx_id: event.tx_id, input_ADA_value: event.input_ADA_value, output_ADA_value: event.output_ADA_value}]->(a2)\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65148c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNet.createOrReplaceTempView(\"TxNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = spark.sql(\"SELECT t1.tx_id, t1.input_id as input1, t2.input_id as input2 FROM TxNet t1 LEFT JOIN TxNet t2 on t1.tx_id = t2.tx_id WHERE t1.input_id != t2.input_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c9a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.createOrReplaceTempView(\"clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11941b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_final = spark.sql(\"SELECT distinct tx_id, input1, input2 FROM clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd9291",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_final.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.210:7687\") \\\n",
    "  .option(\"query\",\"MATCH (a1:Address {id: event.input1}) MATCH (a2:Address {id: event.input2}) CREATE (a1)-[:ASSOCIATED_WITH]->(a2)\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1564462f",
   "metadata": {},
   "source": [
    "### Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed21f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNet = spark.read.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "  \t.option('spark.mongodb.collection', 'neo4j_stream_stage3_5') \\\n",
    "\t.option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    "\t.option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .schema(schema6) \\\n",
    "  \t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206be621",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNet.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.210:7687\") \\\n",
    "  .option(\"query\",\"MATCH (a1:Address {id: event.input_id}) MATCH (a2:Address {id: event.output_id}) CREATE (a1)-[:TRANSACTED_WITH {tx_hash: event.tx_hash, tx_id: event.tx_id, input_ADA_value: event.input_ADA_value, output_ADA_value: event.output_ADA_value}]->(a2)\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bfac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "TxNet.createOrReplaceTempView(\"TxNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9973e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = spark.sql(\"SELECT t1.tx_id, t1.input_id as input1, t2.input_id as input2 FROM TxNet t1 LEFT JOIN TxNet t2 on t1.tx_id = t2.tx_id WHERE t1.input_id != t2.input_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.createOrReplaceTempView(\"clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_final = spark.sql(\"SELECT distinct tx_id, input1, input2 FROM clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_final.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"bolt://172.23.149.210:7687\") \\\n",
    "  .option(\"query\",\"MATCH (a1:Address {id: event.input1}) MATCH (a2:Address {id: event.input2}) CREATE (a1)-[:ASSOCIATED_WITH]->(a2)\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a304f4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-18T15:05:49.894306Z",
     "start_time": "2022-12-18T15:05:49.894295Z"
    }
   },
   "outputs": [],
   "source": [
    "last_ind = db2[\"last_index_neo4j_stream\"]\n",
    "addr_query = {\"collection\": \"neo4j_stream\"}\n",
    "new_count = {\"$set\":{\"last_index\": count_records}}\n",
    "last_ind.update_one(addr_query, new_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd3a337b-af5b-45a4-bbc6-92b46a9ad4c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T23:04:14.631550Z",
     "start_time": "2023-01-07T23:04:13.992383Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
