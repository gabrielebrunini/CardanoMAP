{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c269a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:00:15.567942Z",
     "start_time": "2023-01-16T21:00:13.359492Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql.functions import col, lit, when, from_json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, ArrayType,BooleanType,StructField,LongType\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b08b84e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:00:19.969211Z",
     "start_time": "2023-01-16T21:00:19.966674Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grab Currrent Time Before Running the Code\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0844bfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:00:22.374797Z",
     "start_time": "2023-01-16T21:00:22.352668Z"
    }
   },
   "outputs": [],
   "source": [
    "client = MongoClient(\"172.23.149.210\", 27017)\n",
    "db = client['cardano_bronze']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf029b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:00:42.433120Z",
     "start_time": "2023-01-16T21:00:42.429440Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the needed tables\n",
    "reward = db[\"node.public.reward\"]\n",
    "\n",
    "last_ind = db[\"last_indexes_2_rewards_history\"]\n",
    "\n",
    "# import required temporary collections to overwrite with new data\n",
    "reward_tmp = db[\"reward_temporary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf8d5e04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:00:43.233643Z",
     "start_time": "2023-01-16T21:00:43.231334Z"
    }
   },
   "outputs": [],
   "source": [
    "# insert initial values in checkpoint table\n",
    "#initial_val_reward = { \"collection\": \"reward\", \"last_index\": 0 }\n",
    "#initial_val_pool_hash = { \"collection\": \"pool_hash\", \"last_index\": 0 }\n",
    "#initial_val_stake_address = { \"collection\": \"stake_address\", \"last_index\": 0 }\n",
    "#\n",
    "#last_ind.insert_one(initial_val_reward)\n",
    "#last_ind.insert_one(initial_val_pool_hash)\n",
    "#last_ind.insert_one(initial_val_stake_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9be651c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:00:48.420989Z",
     "start_time": "2023-01-16T21:00:48.416521Z"
    }
   },
   "outputs": [],
   "source": [
    "#retrieve the last indices that were processed before\n",
    "reward_last_processed = last_ind.find_one({'collection': 'reward'})['last_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "869dc36f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:00:57.215791Z",
     "start_time": "2023-01-16T21:00:57.212733Z"
    }
   },
   "outputs": [],
   "source": [
    "# count how many documents are in each new input mongodb collection\n",
    "count_reward = reward.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a444a1ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:01:27.202834Z",
     "start_time": "2023-01-16T21:01:27.199373Z"
    }
   },
   "outputs": [],
   "source": [
    "# for each Cardano table, select the records which haven't been processed yet (range between last_processed and total records count)\n",
    "reward_df = reward.find()[reward_last_processed:count_reward]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23fc6a85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:01:28.564614Z",
     "start_time": "2023-01-16T21:01:28.561365Z"
    }
   },
   "outputs": [],
   "source": [
    "#Â drop the previous records in the temporary collections\n",
    "reward_tmp.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6e32592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:04:50.454980Z",
     "start_time": "2023-01-16T21:01:30.075566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f4ad021fc10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the temporary records in the temporary collections\n",
    "reward_tmp.insert_many(reward_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae3f181",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f3b1830",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:14:49.128439Z",
     "start_time": "2023-01-16T21:14:49.124395Z"
    }
   },
   "outputs": [],
   "source": [
    "config = pyspark.SparkConf().setAll([\n",
    "    #('spark.driver.extraJavaOptions', '-Djava.io.tmpdir=/home/ubuntu/notebook/tmp_spark_env'),\n",
    "    ('spark.executor.memory', '30g'),\n",
    "    ('spark.executor.cores', '3'),\n",
    "    ('spark.cores.max', '3'),\n",
    "    ('spark.driver.memory','10g'),\n",
    "    ('spark.executor.instances', '2'),\n",
    "    ('spark.worker.cleanup.enabled', 'true'),\n",
    "    ('spark.worker.cleanup.interval', '60'),\n",
    "    ('spark.worker.cleanup.appDataTtl', '60'),\n",
    "    ('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector:10.0.2'),\n",
    "    ('spark.mongodb.output.writeConcern.wTimeoutMS','120000'),\n",
    "    ('spark.mongodb.output.writeConcern.socketTimeoutMS','120000'),\n",
    "    ('spark.mongodb.output.writeConcern.connectTimeoutMS','120000')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc20353b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:14:56.484808Z",
     "start_time": "2023-01-16T21:14:50.123574Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/16 22:14:51 WARN Utils: Your hostname, cardano-druid resolves to a loopback address: 127.0.0.1; using 172.23.149.210 instead (on interface ens3)\n",
      "23/01/16 22:14:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5c7deeb4-a416-42b4-89e7-65372bf5b6b7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector;10.0.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.5.1 in central\n",
      "\t[4.5.1] org.mongodb#mongodb-driver-sync;[4.5.0,4.5.99)\n",
      "\tfound org.mongodb#bson;4.5.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.5.1 in central\n",
      ":: resolution report :: resolve 2196ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.5.1 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector;10.0.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   1   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5c7deeb4-a416-42b4-89e7-65372bf5b6b7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/5ms)\n",
      "23/01/16 22:14:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/16 22:14:54 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    " .config(conf=config) \\\n",
    "    .appName(\"MongoDB-rewards-history\") \\\n",
    "    .master(\"spark://172.23.149.210:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d9da0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tables needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1866e6f",
   "metadata": {},
   "source": [
    "- reward\n",
    "- pool_hash\n",
    "- stake_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d81e6622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:14:58.513929Z",
     "start_time": "2023-01-16T21:14:56.487615Z"
    }
   },
   "outputs": [],
   "source": [
    "reward = spark.read.format(\"mongodb\") \\\n",
    " .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'reward_temporary') \\\n",
    " .option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    " .option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fb79cca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:14:59.411896Z",
     "start_time": "2023-01-16T21:14:59.264585Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pool_hash = spark.read.format(\"mongodb\") \\\n",
    " .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'node.public.pool_hash') \\\n",
    " .option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    " .option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70b677b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:16:59.569879Z",
     "start_time": "2023-01-16T21:14:59.973343Z"
    }
   },
   "outputs": [],
   "source": [
    "stake_address = spark.read.format(\"mongodb\") \\\n",
    " .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'node.public.stake_address') \\\n",
    " .option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    " .option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41072f92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:17:07.229268Z",
     "start_time": "2023-01-16T21:17:06.781585Z"
    }
   },
   "outputs": [],
   "source": [
    "reward.createOrReplaceTempView(\"reward\")\n",
    "pool_hash.createOrReplaceTempView(\"pool_hash\")\n",
    "stake_address.createOrReplaceTempView(\"stake_address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b597497",
   "metadata": {},
   "source": [
    "# Get the reward history per address:\n",
    "Meaning: retrieve the rewards by epoch, grouping by address. Delegated pools are also reported in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e89bb52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:17:08.344042Z",
     "start_time": "2023-01-16T21:17:08.340843Z"
    }
   },
   "outputs": [],
   "source": [
    "reward_history_address = \"select stake_address.id, stake_address.view, reward.earned_epoch, \\\n",
    "pool_hash.view as delegated_pool, reward.amount as lovelace \\\n",
    "    from reward \\\n",
    "    inner join stake_address on reward.addr_id = stake_address.id \\\n",
    "    inner join pool_hash on reward.pool_id = pool_hash.id \\\n",
    "    order by earned_epoch desc;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9354cc8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:17:09.164006Z",
     "start_time": "2023-01-16T21:17:09.019788Z"
    }
   },
   "outputs": [],
   "source": [
    "reward_history_address_result = spark.sql(reward_history_address)\n",
    "reward_history_address_result.createOrReplaceTempView(\"reward_history_address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4633f06c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T21:17:58.332898Z",
     "start_time": "2023-01-16T21:17:09.760547Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/16 22:17:09 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Write in the SILVER db, collection rewards_by_address. This is the first step of processing the data to find insights in the rewards history.\n",
    "## Note: the writing takes place as temporary table on Spark\n",
    "reward_history_address_result.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.mode(\"append\") \\\n",
    "    .option('spark.mongodb.database', 'cardano_silver') \\\n",
    "  \t.option('spark.mongodb.collection', 'rewards_history') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22cb332",
   "metadata": {},
   "source": [
    "# Intermediate import of rewards_history table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e11c8fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T22:28:39.571928Z",
     "start_time": "2023-01-16T22:23:39.753010Z"
    }
   },
   "outputs": [],
   "source": [
    "reward_history_address = spark.read.format(\"mongodb\") \\\n",
    " .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .option('spark.mongodb.database', 'cardano_silver') \\\n",
    "   .option('spark.mongodb.collection', 'rewards_history') \\\n",
    " .option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    " .option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5f458b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T22:28:39.627482Z",
     "start_time": "2023-01-16T22:28:39.580501Z"
    }
   },
   "outputs": [],
   "source": [
    "reward_history_address.createOrReplaceTempView(\"reward_history_address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5d477",
   "metadata": {},
   "source": [
    "### Gold database results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5db6d2",
   "metadata": {},
   "source": [
    "__2. Get the total rewards by epoch - rewards_by_epoch__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1336871d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T22:28:47.995470Z",
     "start_time": "2023-01-16T22:28:47.993123Z"
    }
   },
   "outputs": [],
   "source": [
    "rewards_by_epoch = \"select earned_epoch, sum(lovelace)/1000000 as tot_ADA \\\n",
    "from reward_history_address \\\n",
    "group by earned_epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31239bc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T22:28:48.997155Z",
     "start_time": "2023-01-16T22:28:48.849324Z"
    }
   },
   "outputs": [],
   "source": [
    "rewards_by_epoch = spark.sql(rewards_by_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbd0dc1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T22:42:05.740362Z",
     "start_time": "2023-01-16T22:28:49.546796Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/16 23:28:49 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rewards_by_epoch.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'cardano_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'rewards_by_epoch') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06788e3c",
   "metadata": {},
   "source": [
    "__3. Get the total rewards by pool - rewards_by_pool__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0786366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T22:55:52.753867Z",
     "start_time": "2023-01-16T22:55:52.738886Z"
    }
   },
   "outputs": [],
   "source": [
    "rewards_by_pool = \"select delegated_pool, sum(lovelace)/1000000 as tot_ADA \\\n",
    "from reward_history_address \\\n",
    "group by delegated_pool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4558814e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T22:55:53.472832Z",
     "start_time": "2023-01-16T22:55:53.429329Z"
    }
   },
   "outputs": [],
   "source": [
    "rewards_by_pool = spark.sql(rewards_by_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7f034d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T22:58:24.374192Z",
     "start_time": "2023-01-16T22:55:54.001685Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/16 23:55:54 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rewards_by_pool.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'cardano_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'rewards_by_pool') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c0fec",
   "metadata": {},
   "source": [
    "__4. Proportion of top stakers - distribution_whales__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4efdb81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T22:58:24.608698Z",
     "start_time": "2023-01-16T22:58:24.602526Z"
    }
   },
   "outputs": [],
   "source": [
    "distribution_whales = \"select view, sum(lovelace)/1000000 as tot_ADA \\\n",
    "from reward_history_address \\\n",
    "group by view \\\n",
    "order by tot_ADA desc \\\n",
    "limit 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44ce09c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T22:58:24.833664Z",
     "start_time": "2023-01-16T22:58:24.610188Z"
    }
   },
   "outputs": [],
   "source": [
    "distribution_whales = spark.sql(distribution_whales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1619c273",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T23:01:25.006163Z",
     "start_time": "2023-01-16T22:58:24.836493Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/16 23:58:24 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "distribution_whales.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'cardano_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'distribution_whales') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46e86266",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T23:05:40.655746Z",
     "start_time": "2023-01-16T23:01:25.080829Z"
    }
   },
   "outputs": [],
   "source": [
    "# the full REWARD table is now required to compute the rewards by type and sum of ADA rewards by type subqueries\n",
    "reward = spark.read.format(\"mongodb\") \\\n",
    " .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .option('spark.mongodb.database', 'cardano_bronze') \\\n",
    "   .option('spark.mongodb.collection', 'node.public.reward') \\\n",
    " .option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    " .option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d400278f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T23:05:40.706372Z",
     "start_time": "2023-01-16T23:05:40.657534Z"
    }
   },
   "outputs": [],
   "source": [
    "reward.createOrReplaceTempView(\"reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f97a3",
   "metadata": {},
   "source": [
    "__5. Count how many rewards by type__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12f8ba40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T23:05:40.724009Z",
     "start_time": "2023-01-16T23:05:40.708175Z"
    }
   },
   "outputs": [],
   "source": [
    "rewards_by_type = \"select count(*),type from reward group by type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "396d7dbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T23:05:41.082502Z",
     "start_time": "2023-01-16T23:05:40.725404Z"
    }
   },
   "outputs": [],
   "source": [
    "rewards_by_type = spark.sql(rewards_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18e9a570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T23:16:46.396225Z",
     "start_time": "2023-01-16T23:05:41.084843Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/17 00:05:41 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "23/01/17 00:15:50 WARN TaskSetManager: Lost task 276.0 in stage 24.0 (TID 1522) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 38 more\n",
      "\n",
      "23/01/17 00:15:50 WARN TaskSetManager: Lost task 275.0 in stage 24.0 (TID 1521) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:726)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#rewards_by_type.write.format(\"mongodb\") \\\n",
    "#\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "#  \t.mode(\"overwrite\") \\\n",
    "#    .option('spark.mongodb.database', 'cardano_gold') \\\n",
    "#  \t.option('spark.mongodb.collection', 'rewards_by_type') \\\n",
    "#  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "#  \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8d658",
   "metadata": {},
   "source": [
    "__6. Sum ADA rewards by type__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba0e2f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T23:16:46.404957Z",
     "start_time": "2023-01-16T23:16:46.402098Z"
    }
   },
   "outputs": [],
   "source": [
    "sum_rewards_by_type = \"select sum(amount)/1000000 as ADA_amount,type from reward group by type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d076c10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T23:16:46.782675Z",
     "start_time": "2023-01-16T23:16:46.406443Z"
    }
   },
   "outputs": [],
   "source": [
    "sum_rewards_by_type = spark.sql(sum_rewards_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "814fe959",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T23:20:10.918948Z",
     "start_time": "2023-01-16T23:16:46.784765Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/17 00:16:46 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sum_rewards_by_type.write.format(\"mongodb\") \\\n",
    "\t.option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "  \t.mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'cardano_gold') \\\n",
    "  \t.option('spark.mongodb.collection', 'ADA_rewards_by_type') \\\n",
    "  \t.option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "  \t.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ff866a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T23:20:11.238221Z",
     "start_time": "2023-01-16T23:20:10.920658Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16845ddd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T23:20:11.322175Z",
     "start_time": "2023-01-16T23:20:11.240191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x7f4ad0211cd0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the old checkpoints with new ones, based on current document count\n",
    "reward_query = { \"collection\": \"reward\" }\n",
    "new_reward_count = { \"$set\": { \"last_index\": count_reward } }\n",
    "\n",
    "last_ind.update_one(reward_query, new_reward_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33965599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-16T23:20:11.419645Z",
     "start_time": "2023-01-16T23:20:11.324245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8391.45000576973\n"
     ]
    }
   ],
   "source": [
    "# Grab Currrent Time After Running the Code\n",
    "end = time.time()\n",
    "\n",
    "#Subtract Start Time from The End Time\n",
    "total_time = end - start\n",
    "print(\"\\n\"+ str(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb8b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
