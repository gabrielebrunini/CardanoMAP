{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef10dfb1",
   "metadata": {},
   "source": [
    "- transaction hash value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a686fc2-05e9-4009-b7e4-fad5b2579e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-09T16:06:16.956983Z",
     "start_time": "2022-10-09T16:06:13.355072Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import col, lit, when, from_json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, ArrayType\n",
    "import pyspark\n",
    "from ast import literal_eval\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, lit, when, from_json\n",
    "from graphframes import *\n",
    "from delta import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import Row\n",
    "from glob import glob\n",
    "import networkx as nx\n",
    "import pyvis\n",
    "from pyvis.network import Network\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964c3682-a628-470b-af54-af92a1a49a85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:38:25.922524Z",
     "start_time": "2022-09-07T22:38:25.919261Z"
    }
   },
   "outputs": [],
   "source": [
    "config = pyspark.SparkConf().setAll([\n",
    "    ('spark.driver.extraJavaOptions', '-Djava.io.tmpdir=/home/ubuntu/notebook/tmp_spark_files'),\n",
    "    ('spark.executor.memory', '80g'), \n",
    "    ('spark.executor.cores', '6'),\n",
    "    ('spark.cores.max', '30'), \n",
    "    ('spark.driver.memory', '20g'),\n",
    "    ('spark.executor.instances', '5'),\n",
    "    ('spark.worker.cleanup.enabled', 'true'),\n",
    "    ('spark.worker.cleanup.interval', '60'),\n",
    "    ('spark.worker.cleanup.appDataTtl', '60'),\n",
    "    ('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector:10.0.2'),\n",
    "    ('spark.mongodb.output.writeConcern.wTimeoutMS', '120000'),\n",
    "    ('spark.mongodb.output.writeConcern.socketTimeoutMS', '120000'),\n",
    "    ('spark.mongodb.output.writeConcern.connectTimeoutMS', '120000')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b59dfa3-fc55-438f-a27b-362a99247352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:38:40.632219Z",
     "start_time": "2022-09-07T22:38:25.923494Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/07 22:38:28 WARN Utils: Your hostname, cardano-druid resolves to a loopback address: 127.0.0.1; using 172.23.149.210 instead (on interface ens3)\n",
      "22/09/07 22:38:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2d54fd72-aa74-4d77-9d82-d8d97c84a9d9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector;10.0.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.5.1 in central\n",
      "\t[4.5.1] org.mongodb#mongodb-driver-sync;[4.5.0,4.5.99)\n",
      "\tfound org.mongodb#bson;4.5.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.5.1 in central\n",
      ":: resolution report :: resolve 2531ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.5.1 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector;10.0.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   1   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2d54fd72-aa74-4d77-9d82-d8d97c84a9d9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/5ms)\n",
      "22/09/07 22:38:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf=config) \\\n",
    "    .appName(\"MongoDB-query-network\") \\\n",
    "    .master(\"spark://172.23.149.210:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce1439-b807-4ce6-9000-1e80564e0581",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Query: put together transaction in and transaction out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cba5cd2-1cde-499a-86ee-8ec6d1fea714",
   "metadata": {},
   "source": [
    "Tables needed:\n",
    "- tx\n",
    "- tx_in\n",
    "- tx_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ea037-e8c1-465c-9cce-8b7bf8e7155a",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "### schemas & csv tables (not needed?)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3a2ab06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T10:28:39.372522Z",
     "start_time": "2022-09-01T10:28:39.364816Z"
    }
   },
   "source": [
    "schema_tx = StructType([ \\\n",
    "    StructField(\"_id\", StringType(), True), \\\n",
    "    StructField(\"out_sum\", StringType(), True), \\\n",
    "    StructField(\"script_size\", IntegerType(), True), \\\n",
    "    StructField(\"size\", StringType(), True), \\\n",
    "    StructField(\"fee\", StringType(), True), \\\n",
    "    StructField(\"deposit\", StringType(), True), \\\n",
    "    StructField(\"id\", IntegerType(), True), \\\n",
    "    StructField(\"invalid_before\", StringType(), True), \\\n",
    "    StructField(\"valid_contract\", StringType(), True), \\\n",
    "    StructField(\"hash\", StringType(), True), \\\n",
    "    StructField(\"block_id\", StringType(), True), \\\n",
    "    StructField(\"invalid_hereafter\", StringType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b92ee7d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T10:28:39.787275Z",
     "start_time": "2022-09-01T10:28:39.373736Z"
    }
   },
   "source": [
    "schema_tx_in = StructType([ \\\n",
    "    StructField(\"_id\", IntegerType(), True), \\\n",
    "    StructField(\"tx_in_id\", IntegerType(), True), \\\n",
    "    StructField(\"tx_out_index\", IntegerType(), True), \\\n",
    "    StructField(\"tx_out_id\", IntegerType(), True), \\\n",
    "    StructField(\"id\", IntegerType(), True), \\\n",
    "    StructField(\"redeemer_id\", IntegerType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d561ee39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T10:28:39.799320Z",
     "start_time": "2022-09-01T10:28:39.788832Z"
    }
   },
   "source": [
    "schema_tx_out = StructType([ \\\n",
    "    StructField(\"_id\", StringType(), True), \\\n",
    "    StructField(\"address_raw\", StringType(), True), \\\n",
    "    StructField(\"address_has_script\", IntegerType(), True), \\\n",
    "    StructField(\"data_hash\", StringType(), True), \\\n",
    "    StructField(\"address\", StringType(), True), \\\n",
    "    StructField(\"reference_script_id\", IntegerType(), True), \\\n",
    "    StructField(\"stake_address_id\", StringType(), True), \\\n",
    "    StructField(\"index\", StringType(), True), \\\n",
    "    StructField(\"id\", IntegerType(), True), \\\n",
    "    StructField(\"inline_datum_id\", IntegerType(), True), \\\n",
    "    StructField(\"tx_id\", IntegerType(), True), \\\n",
    "    StructField(\"value\", IntegerType(), True), \\\n",
    "    StructField(\"payment_cred\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bc60cc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T18:49:22.564752Z",
     "start_time": "2022-08-31T18:49:20.941721Z"
    },
    "tags": []
   },
   "source": [
    "tx_in = spark.read.json(\"/home/ubuntu/mock_data/shared_data/tx_in_10k.json\")\n",
    "tx_in.createOrReplaceTempView(\"tx_in\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd959c20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T18:49:22.809931Z",
     "start_time": "2022-08-31T18:49:22.566550Z"
    }
   },
   "source": [
    "tx = spark.read.json(\"/home/ubuntu/mock_data/shared_data/tx_10k.json\")\n",
    "tx.createOrReplaceTempView(\"tx\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5023bf4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T18:49:24.297792Z",
     "start_time": "2022-08-31T18:49:22.811482Z"
    },
    "tags": []
   },
   "source": [
    "tx_out = spark.read.json(\"/home/ubuntu/mock_data/shared_data/tx_out_10k.json\")\n",
    "tx_out.createOrReplaceTempView(\"tx_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471177c-c27d-41fc-8db9-6e11a6bb6fed",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import tables from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff18146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:40:32.438787Z",
     "start_time": "2022-09-07T22:38:40.634503Z"
    }
   },
   "outputs": [],
   "source": [
    "tx = spark.read.format(\"mongodb\") \\\n",
    " .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .option('spark.mongodb.database', 'cardano') \\\n",
    "   .option('spark.mongodb.collection', 'node.public.tx') \\\n",
    " .option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    " .option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae4ace3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:42:23.289365Z",
     "start_time": "2022-09-07T22:40:32.441176Z"
    }
   },
   "outputs": [],
   "source": [
    "# massive table, 116.3M documents! Would be interesting to execute the Bokeh plot result and see what happens...\n",
    "tx_in = spark.read.format(\"mongodb\") \\\n",
    " .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .option('spark.mongodb.database', 'cardano') \\\n",
    "   .option('spark.mongodb.collection', 'node.public.tx_in') \\\n",
    " .option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    " .option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51b56bdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:43:18.400876Z",
     "start_time": "2022-09-07T22:42:23.291232Z"
    }
   },
   "outputs": [],
   "source": [
    "tx_out = spark.read.format(\"mongodb\") \\\n",
    " .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .option('spark.mongodb.database', 'cardano') \\\n",
    "   .option('spark.mongodb.collection', 'node.public.tx_out') \\\n",
    " .option('spark.mongodb.read.readPreference.name', 'primaryPreferred') \\\n",
    " .option('spark.mongodb.change.stream.publish.full.document.only','true') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6937408c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:43:19.029073Z",
     "start_time": "2022-09-07T22:43:18.402343Z"
    }
   },
   "outputs": [],
   "source": [
    "tx.createOrReplaceTempView(\"tx\")\n",
    "tx_in.createOrReplaceTempView(\"tx_in\")\n",
    "tx_out.createOrReplaceTempView(\"tx_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c24f4-ee1e-470c-9400-e7a4624e4ebb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transaction outputs for specified transaction hash:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5c86b",
   "metadata": {},
   "source": [
    "First, join the tx_out and tx tables on the respective transaction ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52aaa37-d51a-49b9-bc13-0858922d0d88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:43:19.034299Z",
     "start_time": "2022-09-07T22:43:19.031409Z"
    }
   },
   "outputs": [],
   "source": [
    "qr_out = \" SELECT tx_out.*, \\\n",
    "       tx_out.value \\\n",
    "FROM   tx_out \\\n",
    "       INNER JOIN tx \\\n",
    "               ON tx_out.tx_id = tx.id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b26dcfb1-36cc-4334-b70f-4879c61b9dab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:43:19.044982Z",
     "start_time": "2022-09-07T22:43:19.035461Z"
    }
   },
   "outputs": [],
   "source": [
    "qr3 = \" \\\n",
    "SELECT qr_out.id, \\\n",
    "       qr_out.address AS address_output, \\\n",
    "       tx.HASH \\\n",
    "FROM   qr_out \\\n",
    "       INNER JOIN tx \\\n",
    "               ON tx.id = qr_out.tx_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9917615f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:43:19.063336Z",
     "start_time": "2022-09-07T22:43:19.055283Z"
    }
   },
   "outputs": [],
   "source": [
    "qr_in = \" \\\n",
    "SELECT tx_out.*, \\\n",
    "       tx.HASH AS tx_hash \\\n",
    "FROM   tx_out \\\n",
    "       INNER JOIN tx_in \\\n",
    "               ON tx_out.tx_id = tx_in.tx_out_id \\\n",
    "       INNER JOIN tx \\\n",
    "               ON tx.id = tx_in.tx_in_id \\\n",
    "                  AND tx_in.tx_out_index = tx_out.index \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecf6196e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:43:19.390995Z",
     "start_time": "2022-09-07T22:43:19.064468Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(qr_out).createOrReplaceTempView(\"qr_out\")\n",
    "spark.sql(qr3).createOrReplaceTempView(\"qr3\")\n",
    "spark.sql(qr_in).createOrReplaceTempView(\"qr_in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b5ef4",
   "metadata": {},
   "source": [
    "__Materialize the results and store them in MongoDB Silver db:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01d84681-e31a-4aa6-a2b6-af435c4ecb76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:43:19.439712Z",
     "start_time": "2022-09-07T22:43:19.392538Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qr_out_table = spark.sql(qr_out)\n",
    "qr3_table = spark.sql(qr3)\n",
    "qr_in_table = spark.sql(qr_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "985f8a91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T19:11:12.939966Z",
     "start_time": "2022-09-07T18:34:00.967732Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/07 18:34:01 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Write in the GOLD db, collection smart_contract_calls\n",
    "qr_out_table.write.format(\"mongodb\") \\\n",
    " .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'cardano_silver') \\\n",
    "   .option('spark.mongodb.collection', 'qr_out_table') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48709852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T19:27:54.893368Z",
     "start_time": "2022-09-07T19:11:12.943058Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/07 19:11:13 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Write in the GOLD db, collection smart_contract_calls\n",
    "qr3_table.write.format(\"mongodb\") \\\n",
    " .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'cardano_silver') \\\n",
    "   .option('spark.mongodb.collection', 'qr3_table') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe1299",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-09-07T22:38:48.922Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/07 22:43:19 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n"
     ]
    }
   ],
   "source": [
    "## Write in the GOLD db, collection smart_contract_calls\n",
    "qr_in_table.write.format(\"mongodb\") \\\n",
    " .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'cardano_silver') \\\n",
    "   .option('spark.mongodb.collection', 'qr_in_table') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9293213-a0d7-4163-bd5b-e2cbb767eb6d",
   "metadata": {},
   "source": [
    "joined on data_hash and hash (which both represent the hash identifier of the transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd1edd45-7c6b-4835-b4ed-edaab4bd03ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:59:26.221567Z",
     "start_time": "2022-09-06T20:59:26.219078Z"
    }
   },
   "outputs": [],
   "source": [
    "query_join_out_in = \" \\\n",
    "SELECT qr_in.address          AS input, \\\n",
    "       qr3.address_output     AS output, \\\n",
    "       qr_in.tx_hash, \\\n",
    "         qr_in.VALUE / 1000000 AS ADA_value \\\n",
    "FROM   qr_in \\\n",
    "       LEFT JOIN qr3 \\\n",
    "              ON qr_in.tx_hash = qr3.HASH \\\n",
    "WHERE  qr_in.address != qr3.address_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85f94d24-ca76-45bc-8633-2b9a64e7d3ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:59:28.631490Z",
     "start_time": "2022-09-06T20:59:28.534547Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(query_join_out_in).createOrReplaceTempView(\"query_join_out_in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3428d61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:59:29.617858Z",
     "start_time": "2022-09-06T20:59:29.588685Z"
    }
   },
   "outputs": [],
   "source": [
    "result = spark.sql(query_join_out_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a9d9bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:02:57.090110Z",
     "start_time": "2022-09-06T20:59:32.160927Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/06 20:59:32 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "22/09/06 21:01:25 WARN TaskSetManager: Lost task 130.0 in stage 0.0 (TID 130) (172.23.149.210 executor 0): com.mongodb.MongoSocketReadException: Prematurely reached end of stream\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:112)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/09/06 21:01:25 WARN TaskSetManager: Lost task 129.0 in stage 0.0 (TID 129) (172.23.149.210 executor 1): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.lambda$getMore$1(QueryBatchCursor.java:284)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.executeWithConnection(QueryBatchCursor.java:524)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.getMore(QueryBatchCursor.java:280)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.doHasNext(QueryBatchCursor.java:161)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor$ResourceManager.execute(QueryBatchCursor.java:409)\n",
      "\tat com.mongodb.internal.operation.QueryBatchCursor.hasNext(QueryBatchCursor.java:148)\n",
      "\tat com.mongodb.client.internal.MongoBatchCursorAdapter.hasNext(MongoBatchCursorAdapter.java:54)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 37 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:(125 + 12) / 961][Stage 1:>  (0 + 0) / 204][Stage 2:>  (0 + 0) / 226]\r",
      "22/09/06 21:01:55 WARN TaskSetManager: Lost task 136.0 in stage 0.0 (TID 136) (172.23.149.210 executor 1): com.mongodb.MongoSocketReadException: Exception receiving message\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.translateReadException(InternalStreamConnection.java:707)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:579)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:415)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:342)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:116)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:345)\n",
      "\tat com.mongodb.internal.operation.CommandOperationHelper.createReadCommandAndExecute(CommandOperationHelper.java:230)\n",
      "\tat com.mongodb.internal.operation.CommandOperationHelper.lambda$executeRetryableRead$4(CommandOperationHelper.java:212)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:575)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:574)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:600)\n",
      "\tat com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:573)\n",
      "\tat com.mongodb.internal.operation.CommandOperationHelper.lambda$executeRetryableRead$5(CommandOperationHelper.java:209)\n",
      "\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)\n",
      "\tat com.mongodb.internal.operation.CommandOperationHelper.executeRetryableRead(CommandOperationHelper.java:215)\n",
      "\tat com.mongodb.internal.operation.CommandOperationHelper.executeRetryableRead(CommandOperationHelper.java:195)\n",
      "\tat com.mongodb.internal.operation.AggregateOperationImpl.execute(AggregateOperationImpl.java:195)\n",
      "\tat com.mongodb.internal.operation.AggregateOperation.execute(AggregateOperation.java:306)\n",
      "\tat com.mongodb.internal.operation.AggregateOperation.execute(AggregateOperation.java:46)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:191)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:135)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:92)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.cursor(MongoIterableImpl.java:97)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.getCursor(MongoPartitionReader.java:110)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:109)\n",
      "\tat com.mongodb.internal.connection.SocketStream.read(SocketStream.java:131)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:718)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveMessageWithAdditionalTimeout(InternalStreamConnection.java:576)\n",
      "\t... 49 more\n",
      "\n",
      "22/09/06 21:01:55 WARN TaskSetManager: Lost task 138.0 in stage 0.0 (TID 138) (172.23.149.210 executor 1): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.23.149.210:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:180)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:44)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:144)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:101)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:291)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:183)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:135)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:92)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.cursor(MongoIterableImpl.java:97)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.getCursor(MongoPartitionReader.java:110)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/06 21:02:25 WARN TaskSetManager: Lost task 136.1 in stage 0.0 (TID 147) (172.23.149.210 executor 1): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.23.149.210:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:180)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:44)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:144)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:101)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:291)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:183)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:135)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:92)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.cursor(MongoIterableImpl.java:97)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.getCursor(MongoPartitionReader.java:110)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/09/06 21:02:55 WARN TaskSetManager: Lost task 136.2 in stage 0.0 (TID 159) (172.23.149.210 executor 1): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.23.149.210:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:180)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:44)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:144)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:101)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:291)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:183)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:135)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:92)\n",
      "\tat com.mongodb.client.internal.MongoIterableImpl.cursor(MongoIterableImpl.java:97)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.getCursor(MongoPartitionReader.java:110)\n",
      "\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/09/06 21:02:55 ERROR TaskSetManager: Task 133 in stage 0.0 failed 4 times; aborting job\n",
      "22/09/06 21:02:56 WARN TaskSetManager: Lost task 121.3 in stage 0.0 (TID 169) (172.23.149.210 executor 0): TaskKilled (Stage cancelled)\n",
      "[Stage 0:=======>                                              (125 + 11) / 961]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o122.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 133 in stage 0.0 failed 4 times, most recent failure: Lost task 133.3 in stage 0.0 (TID 168) (172.23.149.210 executor 0): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.23.149.210:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:180)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:44)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:144)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:101)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:291)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:183)\n\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:135)\n\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:92)\n\tat com.mongodb.client.internal.MongoIterableImpl.cursor(MongoIterableImpl.java:97)\n\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.getCursor(MongoPartitionReader.java:110)\n\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.23.149.210:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:180)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:44)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:144)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:101)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:291)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:183)\n\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:135)\n\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:92)\n\tat com.mongodb.client.internal.MongoIterableImpl.cursor(MongoIterableImpl.java:97)\n\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.getCursor(MongoPartitionReader.java:110)\n\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Write in the GOLD db, collection smart_contract_calls\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmongodb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark.mongodb.connection.uri\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmongodb://172.23.149.210:27017\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark.mongodb.database\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcardano_silver\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark.mongodb.collection\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransaction_network\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforceDeleteTempCheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py:738\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o122.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 133 in stage 0.0 failed 4 times, most recent failure: Lost task 133.3 in stage 0.0 (TID 168) (172.23.149.210 executor 0): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.23.149.210:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:180)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:44)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:144)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:101)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:291)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:183)\n\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:135)\n\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:92)\n\tat com.mongodb.client.internal.MongoIterableImpl.cursor(MongoIterableImpl.java:97)\n\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.getCursor(MongoPartitionReader.java:110)\n\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.23.149.210:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:180)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:44)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:144)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:101)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:291)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:183)\n\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:135)\n\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:92)\n\tat com.mongodb.client.internal.MongoIterableImpl.cursor(MongoIterableImpl.java:97)\n\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.getCursor(MongoPartitionReader.java:110)\n\tat com.mongodb.spark.sql.connector.read.MongoPartitionReader.next(MongoPartitionReader.java:75)\n\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:93)\n\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:130)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "## Write in the GOLD db, collection smart_contract_calls\n",
    "result.write.format(\"mongodb\") \\\n",
    " .option('spark.mongodb.connection.uri', 'mongodb://172.23.149.210:27017') \\\n",
    "   .mode(\"overwrite\") \\\n",
    "    .option('spark.mongodb.database', 'cardano_silver') \\\n",
    "   .option('spark.mongodb.collection', 'transaction_network') \\\n",
    "   .option(\"forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc5ff8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:02:57.092911Z",
     "start_time": "2022-09-06T21:02:57.092900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/06 21:02:57 WARN TaskSetManager: Lost task 137.3 in stage 0.0 (TID 177) (172.23.149.210 executor 0): TaskKilled (Stage cancelled)\n",
      "22/09/06 21:02:57 WARN TaskSetManager: Lost task 138.1 in stage 0.0 (TID 172) (172.23.149.210 executor 1): TaskKilled (Stage cancelled)\n",
      "22/09/06 21:02:57 WARN TaskSetManager: Lost task 123.3 in stage 0.0 (TID 173) (172.23.149.210 executor 1): TaskKilled (Stage cancelled)\n",
      "22/09/06 21:02:57 WARN TaskSetManager: Lost task 140.3 in stage 0.0 (TID 178) (172.23.149.210 executor 0): TaskKilled (Stage cancelled)\n",
      "22/09/06 21:02:57 WARN TaskSetManager: Lost task 141.3 in stage 0.0 (TID 179) (172.23.149.210 executor 1): TaskKilled (Stage cancelled)\n",
      "22/09/06 21:02:57 WARN TaskSetManager: Lost task 124.3 in stage 0.0 (TID 170) (172.23.149.210 executor 1): TaskKilled (Stage cancelled)\n",
      "22/09/06 21:02:57 WARN TaskSetManager: Lost task 139.3 in stage 0.0 (TID 180) (172.23.149.210 executor 0): TaskKilled (Stage cancelled)\n",
      "22/09/06 21:02:57 WARN TaskSetManager: Lost task 120.3 in stage 0.0 (TID 176) (172.23.149.210 executor 0): TaskKilled (Stage cancelled)\n",
      "22/09/06 21:02:57 WARN TaskSetManager: Lost task 134.3 in stage 0.0 (TID 175) (172.23.149.210 executor 0): TaskKilled (Stage cancelled)\n",
      "22/09/06 21:02:57 WARN TaskSetManager: Lost task 135.3 in stage 0.0 (TID 174) (172.23.149.210 executor 1): TaskKilled (Stage cancelled)\n",
      "22/09/06 21:02:57 WARN TaskSetManager: Lost task 136.3 in stage 0.0 (TID 171) (172.23.149.210 executor 1): TaskKilled (Stage cancelled)\n",
      "22/09/06 21:15:12 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "22/09/06 21:15:12 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:919)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:154)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:262)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:169)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efb0d528",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T13:58:29.117271Z",
     "start_time": "2022-08-16T13:58:29.117258Z"
    },
    "tags": []
   },
   "source": [
    "# save the network table\n",
    "if path.exists(\"query_results/network_query_spark\"):\n",
    "    shutil.rmtree('query_results/network_query_spark')\n",
    "    spark.sql(query_join_out_in).write.option(\"header\",True).csv(\"query_results/network_query_spark\")\n",
    "else:\n",
    "    spark.sql(query_join_out_in).write.option(\"header\",True).csv(\"query_results/network_query_spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7736ea-c2e7-4309-8284-847037d924f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# network creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe952c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-09T16:06:29.934822Z",
     "start_time": "2022-10-09T16:06:29.643482Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/ubuntu/notebook/query_results/network_query_spark/part-00000-aaa30817-0114-4a88-a6bb-b194614eaa65-c000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3b7f07-d3fd-4e4f-90e5-b15194c4253d",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "### Approach 1: Pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8be80a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-09T16:06:37.000304Z",
     "start_time": "2022-10-09T16:06:36.986557Z"
    }
   },
   "outputs": [],
   "source": [
    "net = Network(notebook = True, directed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a730f51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-09T16:07:39.259299Z",
     "start_time": "2022-10-09T16:06:39.961368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b)0\u001b7\u001b[?47h\u001b[1;24r\u001b[m\u001b[4l\u001b[?1h\u001b=\u001b[m\u001b[m\u001b[37m\u001b[40m\u001b[1;1H                                                                                \u001b[2;1H                                                                                \u001b[3;1H                                                                                \u001b[4;1H                                                                                \u001b[5;1H                                                                                \u001b[6;1H                                                                                \u001b[7;1H                                                                                \u001b[8;1H                                                                                \u001b[9;1H                                                                                \u001b[10;1H                                                                                \u001b[11;1H                                                                                \u001b[12;1H                                                                                \u001b[13;1H                                                                                \u001b[14;1H                                                                                \u001b[15;1H                                                                                \u001b[16;1H                                                                                \u001b[17;1H                                                                                \u001b[18;1H                                                                                \u001b[19;1H                                                                                \u001b[20;1H                                                                                \u001b[21;1H                                                                                \u001b[22;1H                                                                                \u001b[23;1H                                                                                \u001b[24;1H                                                                               \b \b\u001b[4h\u001b[37m\u001b[40m \u001b[4l\u001b[H\u001b[m\u001b[m\u001b[37m\u001b[40m\u001b[m\u001b[m\u001b[21B\u001b[33m\u001b[44m\u001b[1mGetting file://localhost/home/ubuntu/notebook/spark_queries/cardano_network.htm \u001b[22;80H\u001b[m\u001b[m\r\n",
      "\n",
      "\u001b[24;1H\u001b[2J\u001b[?47l\u001b8\r",
      "\u001b[?1l\u001b>"
     ]
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "import pandas as pd\n",
    "\n",
    "df_net = Network(height='750px', width='100%', bgcolor='#222222', font_color='white')\n",
    "\n",
    "# set the physics layout of the network\n",
    "df_net.barnes_hut()\n",
    "\n",
    "sources = df['input']\n",
    "targets = df['output']\n",
    "weights = df['ADA_value']\n",
    "\n",
    "edge_data = zip(sources, targets, weights)\n",
    "\n",
    "for e in edge_data:\n",
    "    src = e[0]\n",
    "    dst = e[1]\n",
    "    w = e[2]\n",
    "\n",
    "    df_net.add_node(src, src, title=src)\n",
    "    df_net.add_node(dst, dst, title=dst)\n",
    "    df_net.add_edge(src, dst, value=w)\n",
    "\n",
    "neighbor_map = df_net.get_adj_list()\n",
    "\n",
    "# add neighbor data to node hover data\n",
    "for node in df_net.nodes:\n",
    "    node['title'] += ' Neighbors:<br>' + '<br>'.join(neighbor_map[node['id']])\n",
    "    node['value'] = len(neighbor_map[node['id']])\n",
    "\n",
    "df_net.show('cardano_network.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a24b7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Approach 2: NetworkX and Bokeh"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2d22711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T13:20:12.521212Z",
     "start_time": "2022-08-16T13:20:12.521203Z"
    }
   },
   "source": [
    "from bokeh.io import output_notebook, show, save\n",
    "from bokeh.models import Range1d, Circle, ColumnDataSource, MultiLine\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.plotting import from_networkx"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf55588e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T13:20:12.521846Z",
     "start_time": "2022-08-16T13:20:12.521837Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 20\n",
    "import numpy as np\n",
    "G = nx.from_pandas_edgelist(df, \"input\", \"output\", \"ADA_value\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e156364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T13:20:12.522541Z",
     "start_time": "2022-08-16T13:20:12.522532Z"
    }
   },
   "source": [
    "#Choose a title!\n",
    "title = 'Cardano Network'\n",
    "\n",
    "#Establish which categories will appear when hovering over each node\n",
    "HOVER_TOOLTIPS = [(\"Node\", \"@index\")]\n",
    "\n",
    "#Create a plot — set dimensions, toolbar, and title\n",
    "plot = figure(tooltips = HOVER_TOOLTIPS,\n",
    "              tools=\"pan,wheel_zoom,save,reset\", active_scroll='wheel_zoom',\n",
    "            x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1), title=title)\n",
    "\n",
    "#Create a network graph object with spring layout\n",
    "# https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.drawing.layout.spring_layout.html\n",
    "network_graph = from_networkx(G, nx.spring_layout, scale=10, center=(0, 0))\n",
    "\n",
    "#Set node size and color\n",
    "network_graph.node_renderer.glyph = Circle(size=15, fill_color='skyblue')\n",
    "\n",
    "#Set edge opacity and width\n",
    "network_graph.edge_renderer.glyph = MultiLine(line_alpha=0.5, line_width=1)\n",
    "\n",
    "#Add network graph to the plot\n",
    "plot.renderers.append(network_graph)\n",
    "\n",
    "#show(plot)\n",
    "save(plot, filename=f\"{title}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b81e07c-b159-4fd8-8940-b4655b4ec183",
   "metadata": {},
   "source": [
    "- objective: build a network of transactions (tx) between nodes (address)\n",
    "- needed tables: tx_out, tx_in, tx\n",
    "\n",
    "- tx_in columns:\n",
    "    - tx_out_id -> The Tx table index of the transaction that contains this transaction input\n",
    "    - tx_in_id -> The Tx table index of the transaction that contains this transaction output\n",
    "- tx columns:\n",
    "    - id -> which transaction contains the \n",
    "- tx_out columns:\n",
    "    - address: from which address the transaction comes (who sent the ADA?)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
